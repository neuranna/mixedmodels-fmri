---
title: "Main analyses"
author: "Idan Blank, Anna Ivanova"
date: "`r Sys.Date()`"
output: html_document
---

# Building your first mixed model

```{r, include=FALSE}
# Loading R packages
library(tidyverse)
library(lme4)
library(lmerTest)
```

```{r, include=FALSE}
# load data
data = read.csv('data/Diachek2020.csv', header=TRUE);
data$Experiment = factor(data$Experiment);
data$System = factor(data$System);
data$Region = factor(data$Region);
data$SubjectID = factor(data$SubjectID);
data$Hemisphere = factor(data$Hemisphere);
data$Modality = factor(data$Modality);
data$Task = factor(data$Task);
data$Condition = factor(data$Condition, levels=c('W', 'S'));
```

```{r, include=FALSE}
data.md = subset(data, (System=="MD"));
  
for (ii in 19:24) {
  x = subset(data.md, Experiment == paste("Experiment", as.character(ii), sep=""));
  if (ii==19){
    data.md.red = x;
  } else {
    data.md.red = rbind(data.md.red,x);
  }
}
```

## Building from the ground up
Let's start with the simplest linear model: only an intercept.
This looks like:
\begin{equation}
\vec{y} = \begin{bmatrix} 1\\...\\1  \end{bmatrix} * \beta_0 + \epsilon
(\#eq:intercept)
\end{equation}

wherÑƒ $\vec{y}$ is a vector with the measurements we want to model (in our example, Effect_Size), $\beta_0$ is the baseline response, and $\epsilon$ is residual error. The length of the ones vector is the same as the number of observations in $\vec{y}$. Essentially, this model is predicting the same number for every single measurement. 

In R syntax, we can say that $\vec{y}$ is proportional to some value (and estimate the coefficients that make this proportion work out). In this case, we assume that each value in $\vec{y}$ is a constant and therefore proportional to 1:

\begin{equation}
y \propto 1
(\#eq:intercept1)
\end{equation}

Here's what it looks like in the code:
```{r}
# specify the model
m.lin.noCond = lm(Effect_Size ~ 1, data = data.md.red)

# show the result
summary(m.lin.noCond)
```

## A model with 1 fixed effect

Let's add a single fixed predictor. Now our effect size $\vec{y}$ depends not only on the baseline response, but also on the experimental condition. The general formula for such a design looks like this:

\begin{equation}
y = X * \vec{\beta} + \epsilon
(\#eq:onebeta)
\end{equation}

What's X? X is the design matrix; it specifies our fixed effects for this model (fixed effects are like regular regression terms - in fact, if you've seen matrix-form regression equations before, this should all be familiar). 

The size of X will depend on the number of conditions. How many conditions do we have here?

```{r}
str(unique(data$Condition))
```

We have 2 conditions, words (W) and sentences (S). So our design matrix will look something like the following:

\begin{equation}
X = \begin{bmatrix} 
    1 & 0 \\
    1 & 0 \\
    ...\\
    1 & 1 \end{bmatrix}
(\#eq:designmatrix1)
\end{equation}

where the first column is our intercept ("baseline" neural activity) and the second column has 1  whenever that row comes from the Sentence condition and 0 otherwise. 

Where is the Word condition?

Remember that we said earlier that the first level of the factor variable gets treated as the baseline. So here, the response to words is our baseline, and the response to sentences is the  stuff you would get "on top" of the word-induced activation  - in other words, the magnitude of  the S>N contrast. The intercept, in turn, would give you the average magnitude of the W>baseline contrast.

This might sound a bit counterintuitive, but imagine if we tried to specify the intercept, word and sentence as 3 separate effects to estimate. Then we could still compare sentences to words, but would have trouble distinguishing the words from the intercept. If our observed response to words is 1.2, does the intercept contribute 0 and words 1.2? intercept 1.2 and words 0? intercept 0.6 and words 0.6? There is an infinite number of combinations.

(Mathematically, we can state that the matrix \@ref(eq:designmatrix1) would be rank-deficient if we added the predictor variable for S, since it's a direct complement of W. Thus, there would be an  infinite number of coefficients $\beta$ for solving equation \@ref(eq:onebeta))

Bottom line: if you have n levels, you can estimate the maximum of n effects.

Ok, with all those preliminaries behind us, let's finally run the model!

```{r}
m.lin = lm(Effect_Size ~ Condition, data = data.md.red)
summary(m.lin)
```

We do not need to explicitly specify an intercept: a model with +1 is the same as a model without it
```{r}
m.lin.int = lm(Effect_Size ~ 1 + Condition, data = data.md.red);
summary(m.lin.int)
```

We can explicitly supress the intercept. The resulting model is the same in terms of its predicted y values, but the coefficients and their interpretation are different. 

THINK: Which two contrasts does this no-intercept model evaluate? (Hint: you can compare these numbers with the numbers produced by the previous model)

```{r}
m.lin.noInt = lm(Effect_Size ~ 0 + Condition, data = data.md.red);
summary(m.lin.noInt)
```


## Was it worth it? Model comparison

Does the model with Condition as a fixed effect explain more variance than the intercept-only model? To find out, we can compare the two models using the anova function. This function implements the [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test).

NOTE: the likelihood ratio (LR) test only works for nested models, i.e. models that have the same structure except that one model has some additional terms. The LR test would then tell us whether these terms are worth adding - in formal words, whether they significantly improve model fit. If you want to compare non-nested models, you can use other methods, such as [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion).
```{r}
anova(m.lin.noCond, m.lin)
```

## Adding random intercepts
### Random intercept #1: participant

Now we're ready to actually build a mixed effect model. 

Our previous model assumed that all our participants had the same response to the S and W conditions - any variation was put in the error term. However, we know that some participants tend to have generally high responses and vice versa; thus, we might expect someone with a really high response to words to also have a high response to sentences. Can we incorporate that knowledge into the model?

We can! Let's tell the model that the intercept varies across participants by adding an additional term to it:

\begin{equation}
y = X * \vec{\beta} + \begin{bmatrix} 1\\...\\1  \end{bmatrix} * \vec{b} + \epsilon
(\#eq:mixedmodel1)
\end{equation}

Here, $\vec{b}$ specifies a participant-specific offset in the response strength - some will be above average, and some below average. This offset is not condition-specific (yet). 

Why do we call it a random effect? Unlike Condition, each $\vec{b}$ value is participant-specific. Still, we could just add a bunch of columns to X for each participant and put 1's against all trials completed by any particular participant. Then our participant-specific estimates would just be added to $\vec{\beta}$. 

In some cases, modeling participant-specific variation as a fixed effect might be a valid approach - for instance, if you have 3 participants and a ton of data for each. However, in our case, we have many participants but not much participant-specific data, so we would just get a lot of noisy effects. Moreover, all the-participant specific terms have something in common - they are not fully independent. For example, if your participant intercepts are 1, 1.1, 0.9, 1.2, you have a pretty strong hunch that the next one is not going to be 15. Mathematically, we can state that participant-specific intercepts come from a normal distribution where the mean is the group intercept (estimated as the first term in $\vec{\beta}$), and the variance is a free parameter we need to estimate. 

Thus, our $\vec{b}$ term is never evaluated directly! It is a random variable of the form 

\begin{equation}
\vec{b} \sim \mathcal{N}(0,\sigma^{2})
\end{equation}

(the mean is 0 because this term is only estimating the participant-specific *offset* from the overall intercept)

With this 'random effect' trick, we achieve three goals:

1. We estimate the intercept effects across all participants by specifying just one parameter - $\sigma^{2}$.

2. We leverage the power of all the dataset, not just the participant-specific data. Thus if any one participant's values are unusually high, it would not have a strong result on the model - the normal distribution will "regularize" the estimates for that participant.

3. We make *generalizable* inferences: instead of estimating the effects for each specific participant, we produce an estimate for the entire population.

LET'S RUN OUR FIRST MIXED EFFECT MODEL!

The previous models did not have any random terms, and so we evaluated it using the 'lm' function. For mixed models, we use 'lmer'.

```{r}
# I want to evaluate the model by maximum likelihood, not restricted maximum likelihood (REML),
# so I'm setting REML to FALSE
m.ri1 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID), data = data.md.red,
             REML=FALSE);
summary(m.ri1)
```

Note that the fixed effects structure is the same as before, but now we also have the "random effects" section, which tells us how much variance was explained by the intercept varying across participants and how much is left in the residuals.

The fixed effect estimates themselves have changed - adding the random effect changes the way the model is fitted to the data.

Does this random effect add to explained variance compared to the fixed-effect-only model?
```{r}
m.noR = m.lin.noInt;
anova(m.ri1, m.noR)
```

### Random intercept #2: experiment 

Another way to think about random effects is that they specify additional clusters in the data. For instance, most datapoints from participant A will be shifted above the mean, while most datapoints from participant B will be below the mean. Points A and points B will form two different clusters. 

The dataset we're using has another obvious source of clusters - it was collected across multiple experiments. Although we would hope that the data we record would be unaffected by the specific experimental conditions, in practice this is rarely the case. The data might be collected on different MRI scanners, during different seasons, the Sentence & Word blocks might have had different lengths, etc. If you have all this information and you think that each variable might affect your results, then you can go ahead and enter these terms directly. However, we here will just use Experiment.

We assume that the mean responses to both conditions will vary across experiments (and, as before, across participants).

To save space, from now on we will only print out fixed and random effect estimates.
```{r}
m.ri2 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment), data = data.md.red,
             REML=FALSE);
coef(summary(m.ri2))    # fixed
VarCorr(m.ri2)          # random
```

Previous model (m.ri1) reminder:
ConditionW  0.47423
ConditionS  0.36225 

We see again that changing the random effect structure affects the fixed effects too.

Let's compare our models to see if adding the experiment intercept actually improved model fit.
```{r}
anova(m.ri1, m.ri2)
```

It did! We have a significant difference between the models, with model 2 having a higher log likelihood of the data.


### Random intercept #3: region 

Finally, we know that different brain regions have different levels of BOLD activity: regions close to large vessels will have strong responses to everything.

```{r}
m.ri3 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + (1 | Region), 
             data = data.md.red, REML=FALSE);
coef(summary(m.ri3))    # fixed
VarCorr(m.ri3)          # random
```

Previous model (m.ri2) reminder:
ConditionW  0.4563056
ConditionS  0.3443295

This time, the fixed effect estimates remained almost the same.

But did adding a random intercept by region improve model fit further? Yes, it did.

```{r}
anova(m.ri2, m.ri3)
```

## It gets better - random slopes {#randomslopes}

Ok, our model is getting pretty complicated. But as you were reading about all those random intercepts, you might have been wondering - what if it's not just the mean activity that varies across participants/regions/experiments? What if participant A not only has stronger activity overall but also has stronger responses to Sentences specifically?

This kind of structure can be captured too. We call it a random *slope* because if we plot Condition on the x axis and Response on the y axis, the effect of interest will be the slope of the line going from the W value to the S value. High slope = steeper increase in activation as we go from W to S.

Remember that so far we have been specifying our random effects as (1 | RandomVar). What does this mean? It means, take the random effect on the right and estimate how it varies according to the fixed effects on the left. So far we've just had 1 on the left, which is our intercept. But now we also want to say that our random variables can vary by condition, so we add it to the left side too.

```{r}
m.ris = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red, REML=FALSE);
```

Uh oh, what happened here? We get a warning saying that our model failed to converge. That's not good - it means that our effect estimates might be imprecise.

What can we do to solve this? There are a few technical tricks we can use to force the model to converge (see section \@ref(convergence)). But generally, the reason why our model here doesn't converge is because it's too complicated - we have too many variables and not enough data. So for now, let's simplify and just add one random slope.

```{r}
m.ris = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE);
coef(summary(m.ris))    # fixed
VarCorr(m.ris)          # random
```

Note that the model also estimates the covariance between the random effect's intercept and slope (in essence, an extra variable to take care of). Here, the intercept and the slope are correlated: participants with higher overall responses will also have a higher S>W value.

THINK: how can we modify equation \@ref(eq:mixedmodel1) to incorporate the random slope by condition?

Let's check: did adding a random slope help beyond random intercepts? It did.
```{r}
anova(m.ris, m.ri3)
```

Note that here I decided to only keep the random slope by participant, and it happened to actually be helpful. If possible, you should be more systematic when deciding on model structure. For details, see the next chapter.


## Fixed effects can be more complicated too

I will leave you with a final twist on our model.

So far we've only looked at Condition as our fixed effect: our goal was to get to random effects as soon as possible. But there is another important source of variation in our data - Hemisphere.

Since there are only two hemispheres, there is no reason to include it as a fixed effect (in this case, we'd have to imagine left and right hemispheres as samples from an infinite population of hemispheres...). So this is going to be our second fixed effect.

```{r}
m.ris2 = lmer(Effect_Size ~ 0 + Condition + Hemisphere + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE);
coef(summary(m.ris2))    # fixed
VarCorr(m.ris2)          # random
```

Great, now we know that left and right hemisphere do not have significantly different overall activity levels^[during word reading - see section \@ref(interactionsV2)]. But we also want to know whether our Condition effects vary by Hemisphere. What do we need to include? An interaction.

```{r}
m.ris3 = lmer(Effect_Size ~ 0 + Condition*Hemisphere + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE);
coef(summary(m.ris3))    # fixed
VarCorr(m.ris3)          # random
```

Here, * is a shortcut meaning "estimate the main effects and the interaction". Thus `Condition*Hemisphere` is equivalent to `Condition + Hemisphere + Condition:Hemisphere`, where `:` stands for the interaction itself. 

And hey, looks like the Sentence responses in the right hemisphere MD regions are weaker than in the left. 

Let's do a final formal check to see whether adding the hemisphere terms was useful.

```{r}
anova(m.ris3, m.ris)
```

Yup, we explained additional variance.

## Summary

- Fixed effects in a linear mixed effect model act just like regular regression terms.
- For categorical variables, the first level of a fixed effect variable acts like a baseline (unless you set the intercept to 0).
- Random effects are not simple regression terms; they are random variables. As such, you don't estimate a value for each level (e.g. mean activation for each participant) but instead find the variance around the mean. For instance, you might estimate how much each participant's data are likely to deviate from the mean value.
- You can estimate random intercepts (how much does a random effect influence the baseline) and random slopes (how much does a random effect modulate a fixed effect)
- You can decide whether adding a particular term improves model fit by conducting a likelihood ratio test. 