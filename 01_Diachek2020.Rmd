---
title: "Diachek et al. Critical analyses"
author: "Idan Blank"
date: "10/13/2020"
output: html_document
---

# Building your first mixed model

```{r}
# Loading R packages
# BEGINNNER TIP: before loading the package for the first time, you need to install it 
# using the command install.packages("myPackageNameInQuotes")
library(bookdown)
library(lme4)
library(multcomp)
library(tidyverse)
library(lmerTest)
library(readxl)
library(optimx)

# Checking the version of the "lme4" package
packageVersion("lme4")
```

## Load data

```{r}
# load data
data = read.csv('data/Diachek2020.csv', header=TRUE);
print(str(data))

# Explicitly specifying all categorical variables as factors. 
# Generally, R will default to treating numbers as type "numeric" and strings as factor, but it helps to be explicit
# COMMON MISTAKE: if participant IDs are numbers, they will be treating as numeric 
# (i.e. participant 2 > participant 1)
data$Experiment = factor(data$Experiment);
data$System = factor(data$System);
data$Region = factor(data$Region);
data$SubjectID = factor(data$SubjectID);
data$Hemisphere = factor(data$Hemisphere);
data$Modality = factor(data$Modality);
data$Task = factor(data$Task);
```

You can also use the 'factor' command to specify the order of the different values ("levels") thata given variable can take. If you don't specify the order, the levels will be ordered alphabetically.

Why does it matter? The first level will be treated as the reference level (baseline) by the lme model. See below and the "contrasts" section for more details.

```{r}
data$Condition = factor(data$Condition, levels=c('W', 'S'));
```

Let's inspect our data!

```{r}
summary(data)
```

To simplify things a bit, we will create a reduced dataset of MD only, and another with only experiments 19-24

```{r}
data.md = subset(data, (System=="MD"));
  
for (ii in 19:24) {
  x = subset(data.md, Experiment == paste("Experiment", as.character(ii), sep=""));
  if (ii==19){
    data.md.red = x;
  } else {
    data.md.red = rbind(data.md.red,x);
  }
}
summary(data.md.red)
```

## Building from the ground up
Let's start with the simplest linear model: only an intercept.
This looks like:
\begin{equation}
\vec{y} = \begin{bmatrix} 1\\...\\1  \end{bmatrix} * \beta_0 + \epsilon
(\#eq:intercept)
\end{equation}

wherÑƒ $\vec{y}$ is a vector with the measurements we want to model (in our example, Effect_Size), $\beta_0$ is the baseline response, and $\epsilon$ is residual error. The length of the ones vector is the same as the number of observations in $\vec{y}$. Essentially, this model is predicting the same number for every single measurement. 

In R syntax, we can say that $\vec{y}$ is proportional to some value (and estimate the coefficients that make this proportion work out). In this case, we assume that each value in $\vec{y}$ is a constant and therefore proportional to 1:

\begin{equation}
y \propto 1
(\#eq:intercept1)
\end{equation}

Here's what it looks like in the code:
```{r}
# specify the model
m.lin.noCond = lm(Effect_Size ~ 1, data = data.md.red)

# show the result
summary(m.lin.noCond)
```

## A model with 1 fixed effect

Let's add a single fixed predictor. Now our effect size $\vec{y}$ depends not only on the baseline response, but also on the experimental condition. The general formula for such a design looks like this:

\begin{equation}
y = X * \vec{\beta} + \epsilon
(\#eq:onebeta)
\end{equation}

What's X? X is the design matrix; it specifies our fixed effects for this model (fixed effects are like regular regression terms - in fact, if you've seen matrix-form regression equations before, this should all be familiar). 

The size of X will depend on the number of conditions. How many conditions do we have here?

```{r}
str(unique(data$Condition))
```

We have 2 conditions, words (W) and sentences (S). So our design matrix will look something like the following:

\begin{equation}
X = \begin{bmatrix} 
    1 & 0 \\
    1 & 0 \\
    ...\\
    1 & 1 \end{bmatrix}
(\#eq:designmatrix1)
\end{equation}

where the first column is our intercept ("baseline" neural activity) and the second column has 1  whenever that row comes from the sentence condition and 0 otherwise. 

Where is the word condition?

Remember that we said earlier that the first level of the factor variable gets treated as the baseline. So here, the response to words is our baseline, and the response to sentences is the  stuff you would get "on top" of the word-induced activation  - in other words, the magnitude of  the S>N contrast. The intercept, in turn, would give you the average magnitude of the W>baseline contrast.

This might sound a bit counterintuitive, but imagine if we tried to specify the intercept, word and sentence as 3 separate effects to estimate. Then we could still compare sentences to words, but would have trouble distinguishing the words from the intercept. If our observed response to words is 1.2, does the intercept contribute 0 and words 1.2? intercept 1.2 and words 0? intercept 0.6 and words 0.6? There is an infinite number of combinations.

(Mathematically, we can state that the matrix \@ref(eq:designmatrix1) would be rank-deficient if we added the predictor variable for S, since it's a direct complement of W. Thus, there would be an  infinite number of coefficients $\beta$ for solving equation \@ref(eq:onebeta))

Bottom line: if you have n levels, you can estimate the maximum of n effects.

Ok, with all those preliminaries behind us, let's finally run the model!

```{r}
m.lin = lm(Effect_Size ~ Condition, data = data.md.red)
summary(m.lin)
```

We do not need to explicitly specify an intercept: a model with +1 is the same as a model without it
```{r}
m.lin.int = lm(Effect_Size ~ 1 + Condition, data = data.md.red);
summary(m.lin.int)
```

We can explicitly supress the intercept. The resulting model is the same in terms of its predicted y values, but the coefficients and their interpretation are different. 

THINK: Which two contrasts does this no-intercept model evaluate? (Hint: you can compare these numbers with the numbers produced by the previous model)

```{r}
m.lin.noInt = lm(Effect_Size ~ 0 + Condition, data = data.md.red);
summary(m.lin.noInt)
```


## Was it worth it? Model comparison

Does the model with Condition as a fixed effect explain more variance than the intercept-only model? To find out, we can compare the two models using the anova function. This function implements the [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test).

NOTE: the likelihood ratio (LR) test only works for nested models, i.e. models that have the same structure except that one model has some additional terms. The LR test would then tell us whether these terms are worth adding - in formal words, whether they significantly improve model fit. If you want to compare non-nested models, you can use other methods, such as [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion).
```{r}
anova(m.lin.noCond, m.lin)
```


## Adding the first random effect: participant

Now we're ready to actually build a mixed effect model. 

Our previous model assumed that all our participants had the same response to the S and W conditions - any variation was put in the error term. However, we know that some participants tend to have generally high responses and vice versa; thus, we might expect someone with a really high response to words to also have a high response to sentences. Can we incorporate that knowledge into the model?

We can! Let's tell the model that the intercept varies across participants by adding an additional term to it:

\begin{equation}
y = X * \vec{\beta} + \begin{bmatrix} 1\\...\\1  \end{bmatrix} * \vec{b} + \epsilon
(\#eq:mixedmodel1)
\end{equation}

Here, $\vec{b}$ specifies a participant-specific offset in the response strength - some will be above average, and some below average. This offset is not condition-specific (yet). 

Why do we call it a random effect? Unlike Condition, each $\vec{b}$ value is participant-specific. Still, we could just add a bunch of columns to X for each participant and put 1's against all trials completed by any particular participant. Then our participant-specific estimates would just be added to $\vec{\beta}$. 

In some cases, modeling participant-specific variation as a fixed effect is a valid approach. However, in our case, we have many participants but not much participant-specific data, so we would just get a lot of noisy effects. Moreover, all the-participant specific terms have something in common - they are not fully independent. For example, if your participant intercepts are 1, 1.1, 0.9, 1.2, you have a pretty strong hunch that the next one is not going to be 15. Mathematically, we can state that participant-specific intercepts come from a normal distribution where the mean is the group intercept (estimated as the first term in $\vec{\beta}$), and the variance is a free parameter we need to estimate. 

Thus, our $\vec{b}$ term is never evaluated directly! It is a random variable of the form 

\begin{equation}
\vec{b} \sim \mathcal{N}(0,\sigma^{2})
\end{equation}

(the mean is 0 because this term is only estimating the participant-specific *offset* from the overall intercept)

With this 'random effect' trick, we achieve two goals:
- we estimate the intercept effects across all participants by specifying just one parameter - \sigma^{2}
- we leverage the power of all the dataset, not just the participant-specific data. Thus if any one participant's values are unusually high, it would not have a strong result on the model - the normal distribution will "regularize" the estimates for that participant.


LET'S RUN OUR FIRST MIXED EFFECT MODEL!

The previous models did not have any random terms, and so we evaluated it using the 'lm' function. For mixed models, we use 'lmer'.

```{r}
# I want to evaluate the model by maximum likelihood, not restricted maximum likelihood (REML),
# so I'm setting REML to FALSE
m.ri1 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID), data = data.md.red,
             REML=FALSE);
summary(m.ri1)
```

Note that the fixed effects structure is the same as before, but now we also have the "random effects" section, which tells us how much variance was explained by the intercept varying across participants and how much is left in the residuals.

The fixed effect estimates themselves have changed - adding the random effect changes the way the model is fitted to the data.

Does this random effect add to explained variance compared to the fixed-effect-only model?
```{r}
m.noR = m.lin.noInt;
anova(m.ri1, m.noR)
```

### Now add random intercepts by experiment ### 
Baseline model reminder: 
ConditionS  0.36225 
ConditionW  0.47423
```{r}
m.ri2 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment), data = data.md.red);
summary(m.ri2)

anova(m.ri1, m.ri2)
```

### Alternatively add random intercepts by region ### 

```{r}
m.ri3 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Region), data = data.md.red);
summary(m.ri3)

anova(m.ri1, m.ri3)
```

### Now add random intercepts by both experiment and region ### 

```{r}
m.ri4 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red);
summary(m.ri4)
```

### Does a random intercept by region improve the model beyond one with participant & experiment?
```{r}
anova(m.ri2, m.ri4)
```

### Does a random intercept by experiment improve the model beyond one with participant & region?
```{r}
anova(m.ri3, m.ri4)
```

### Now add random slopes

```{r}
m.ris = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red);
summary(m.ris)
```

### Do random slopes help beyond random intercepts?
```{r}
anova(m.ris, m.ri4)
```

### We also have hemisphere as a variable in our model ### 
### Note that below it's not included in the random structure, although it could / should be ###
```{r}
m.ris2 = lmer(Effect_Size ~ 0 + Condition*Hemisphere + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red);
summary(m.ris2)
```
