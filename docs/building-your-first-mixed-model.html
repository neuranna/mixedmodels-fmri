<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Building your first mixed model | Mixed Effect Models</title>
  <meta name="description" content="3 Building your first mixed model | Mixed Effect Models" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Building your first mixed model | Mixed Effect Models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Building your first mixed model | Mixed Effect Models" />
  
  
  

<meta name="author" content="Compiled by Anna Ivanova based on materials by Idan Blank, Rachel Ryskin, and Cory Shain" />


<meta name="date" content="2020-12-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="meet-the-data.html"/>
<link rel="next" href="the-devils-in-the-details.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION</a></li>
<li class="chapter" data-level="2" data-path="meet-the-data.html"><a href="meet-the-data.html"><i class="fa fa-check"></i><b>2</b> Meet the Data</a><ul>
<li class="chapter" data-level="2.1" data-path="meet-the-data.html"><a href="meet-the-data.html#setting-up-the-environment"><i class="fa fa-check"></i><b>2.1</b> Setting up the environment</a></li>
<li class="chapter" data-level="2.2" data-path="meet-the-data.html"><a href="meet-the-data.html#the-data"><i class="fa fa-check"></i><b>2.2</b> The data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html"><i class="fa fa-check"></i><b>3</b> Building your first mixed model</a><ul>
<li class="chapter" data-level="3.1" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#building-from-the-ground-up"><i class="fa fa-check"></i><b>3.1</b> Building from the ground up</a></li>
<li class="chapter" data-level="3.2" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#a-model-with-1-fixed-effect"><i class="fa fa-check"></i><b>3.2</b> A model with 1 fixed effect</a></li>
<li class="chapter" data-level="3.3" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#was-it-worth-it-model-comparison"><i class="fa fa-check"></i><b>3.3</b> Was it worth it? Model comparison</a></li>
<li class="chapter" data-level="3.4" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#random-effect-1-participant"><i class="fa fa-check"></i><b>3.4</b> Random effect #1: participant</a></li>
<li class="chapter" data-level="3.5" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#random-intercept-2-experiment"><i class="fa fa-check"></i><b>3.5</b> Random intercept #2: experiment</a></li>
<li class="chapter" data-level="3.6" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#random-intercept-3-region"><i class="fa fa-check"></i><b>3.6</b> Random intercept #3: region</a></li>
<li class="chapter" data-level="3.7" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#it-gets-better---random-slopes"><i class="fa fa-check"></i><b>3.7</b> It gets better - random slopes</a></li>
<li class="chapter" data-level="3.8" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#fixed-effects-can-be-more-complicated-too"><i class="fa fa-check"></i><b>3.8</b> Fixed effects can be more complicated too</a></li>
<li class="chapter" data-level="3.9" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#summary"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html"><i class="fa fa-check"></i><b>4</b> The devil’s in the details</a><ul>
<li class="chapter" data-level="4.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#testing-significance"><i class="fa fa-check"></i><b>4.1</b> Testing significance</a><ul>
<li class="chapter" data-level="4.1.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#method-1-read-the-p-values-directly-from-model-summary"><i class="fa fa-check"></i><b>4.1.1</b> Method 1: Read the p-values directly from model summary</a></li>
<li class="chapter" data-level="4.1.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#method-2-model-comparison"><i class="fa fa-check"></i><b>4.1.2</b> Method 2: Model comparison</a></li>
<li class="chapter" data-level="4.1.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#method-3-pairwise-tests"><i class="fa fa-check"></i><b>4.1.3</b> Method 3: pairwise tests</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#contrasts"><i class="fa fa-check"></i><b>4.2</b> Contrasts</a><ul>
<li class="chapter" data-level="4.2.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#intro-to-contrast-coding"><i class="fa fa-check"></i><b>4.2.1</b> Intro to contrast coding</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#interactionsV2"><i class="fa fa-check"></i><b>4.2.2</b> Applications to interactions</a></li>
<li class="chapter" data-level="4.2.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#interpretation"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#convergence-issues"><i class="fa fa-check"></i><b>4.3</b> Convergence issues</a></li>
<li class="chapter" data-level="4.4" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-1-try-other-optimizers"><i class="fa fa-check"></i><b>4.4</b> Solution 1: try other optimizers</a><ul>
<li class="chapter" data-level="4.4.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-2-brms"><i class="fa fa-check"></i><b>4.4.1</b> Solution 2: brms</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-3-reml"><i class="fa fa-check"></i><b>4.4.2</b> Solution 3: REML</a></li>
<li class="chapter" data-level="4.4.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-4-simplify-your-model"><i class="fa fa-check"></i><b>4.4.3</b> Solution 4: simplify your model</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mixed Effect Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="building-your-first-mixed-model" class="section level1">
<h1><span class="header-section-number">3</span> Building your first mixed model</h1>
<div id="building-from-the-ground-up" class="section level2">
<h2><span class="header-section-number">3.1</span> Building from the ground up</h2>
<p>Let’s start with the simplest linear model: only an intercept.
This looks like:
<span class="math display" id="eq:intercept">\[\begin{equation}
\vec{y} = \begin{bmatrix} 1\\...\\1  \end{bmatrix} * \beta_0 + \epsilon
\tag{3.1}
\end{equation}\]</span></p>
<p>wherу <span class="math inline">\(\vec{y}\)</span> is a vector with the measurements we want to model (in our example, Effect_Size), <span class="math inline">\(\beta_0\)</span> is the baseline response, and <span class="math inline">\(\epsilon\)</span> is residual error. The length of the ones vector is the same as the number of observations in <span class="math inline">\(\vec{y}\)</span>. Essentially, this model is predicting the same number for every single measurement.</p>
<p>In R syntax, we can say that <span class="math inline">\(\vec{y}\)</span> is proportional to some value (and estimate the coefficients that make this proportion work out). In this case, we assume that each value in <span class="math inline">\(\vec{y}\)</span> is a constant and therefore proportional to 1:</p>
<p><span class="math display" id="eq:intercept1">\[\begin{equation}
y \propto 1
\tag{3.2}
\end{equation}\]</span></p>
<p>Here’s what it looks like in the code:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># specify the model</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">m.lin.noCond =<span class="st"> </span><span class="kw">lm</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> data.md.red)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="co"># show the result</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="kw">summary</span>(m.lin.noCond)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Effect_Size ~ 1, data = data.md.red)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2640 -0.4187 -0.0466  0.3720  3.6304 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.41824    0.01044   40.08   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.717 on 4719 degrees of freedom</code></pre>
</div>
<div id="a-model-with-1-fixed-effect" class="section level2">
<h2><span class="header-section-number">3.2</span> A model with 1 fixed effect</h2>
<p>Let’s add a single fixed predictor. Now our effect size <span class="math inline">\(\vec{y}\)</span> depends not only on the baseline response, but also on the experimental condition. The general formula for such a design looks like this:</p>
<p><span class="math display" id="eq:onebeta">\[\begin{equation}
y = X * \vec{\beta} + \epsilon
\tag{3.3}
\end{equation}\]</span></p>
<p>What’s X? X is the design matrix; it specifies our fixed effects for this model (fixed effects are like regular regression terms - in fact, if you’ve seen matrix-form regression equations before, this should all be familiar).</p>
<p>The size of X will depend on the number of conditions. How many conditions do we have here?</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">str</span>(<span class="kw">unique</span>(data<span class="op">$</span>Condition))</a></code></pre></div>
<pre><code>##  Factor w/ 2 levels &quot;W&quot;,&quot;S&quot;: 2 1</code></pre>
<p>We have 2 conditions, words (W) and sentences (S). So our design matrix will look something like the following:</p>
<p><span class="math display" id="eq:designmatrix1">\[\begin{equation}
X = \begin{bmatrix} 
    1 &amp; 0 \\
    1 &amp; 0 \\
    ...\\
    1 &amp; 1 \end{bmatrix}
\tag{3.4}
\end{equation}\]</span></p>
<p>where the first column is our intercept (“baseline” neural activity) and the second column has 1 whenever that row comes from the Sentence condition and 0 otherwise.</p>
<p>Where is the Word condition?</p>
<p>Remember that we said earlier that the first level of the factor variable gets treated as the baseline. So here, the response to words is our baseline, and the response to sentences is the stuff you would get “on top” of the word-induced activation - in other words, the magnitude of the S&gt;N contrast. The intercept, in turn, would give you the average magnitude of the W&gt;baseline contrast.</p>
<p>This might sound a bit counterintuitive, but imagine if we tried to specify the intercept, word and sentence as 3 separate effects to estimate. Then we could still compare sentences to words, but would have trouble distinguishing the words from the intercept. If our observed response to words is 1.2, does the intercept contribute 0 and words 1.2? intercept 1.2 and words 0? intercept 0.6 and words 0.6? There is an infinite number of combinations.</p>
<p>(Mathematically, we can state that the matrix <a href="building-your-first-mixed-model.html#eq:designmatrix1">(3.4)</a> would be rank-deficient if we added the predictor variable for S, since it’s a direct complement of W. Thus, there would be an infinite number of coefficients <span class="math inline">\(\beta\)</span> for solving equation <a href="building-your-first-mixed-model.html#eq:onebeta">(3.3)</a>)</p>
<p>Bottom line: if you have n levels, you can estimate the maximum of n effects.</p>
<p>Ok, with all those preliminaries behind us, let’s finally run the model!</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">m.lin =<span class="st"> </span><span class="kw">lm</span>(Effect_Size <span class="op">~</span><span class="st"> </span>Condition, <span class="dt">data =</span> data.md.red)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">summary</span>(m.lin)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Effect_Size ~ Condition, data = data.md.red)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2080 -0.4233 -0.0426  0.3806  3.5744 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.47423    0.01471  32.228  &lt; 2e-16 ***
## ConditionS  -0.11198    0.02081  -5.381 7.77e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7148 on 4718 degrees of freedom
## Multiple R-squared:  0.0061, Adjusted R-squared:  0.005889 
## F-statistic: 28.95 on 1 and 4718 DF,  p-value: 7.77e-08</code></pre>
<p>We do not need to explicitly specify an intercept: a model with +1 is the same as a model without it</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">m.lin.int =<span class="st"> </span><span class="kw">lm</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition, <span class="dt">data =</span> data.md.red);</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">summary</span>(m.lin.int)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Effect_Size ~ 1 + Condition, data = data.md.red)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2080 -0.4233 -0.0426  0.3806  3.5744 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.47423    0.01471  32.228  &lt; 2e-16 ***
## ConditionS  -0.11198    0.02081  -5.381 7.77e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7148 on 4718 degrees of freedom
## Multiple R-squared:  0.0061, Adjusted R-squared:  0.005889 
## F-statistic: 28.95 on 1 and 4718 DF,  p-value: 7.77e-08</code></pre>
<p>We can explicitly supress the intercept. The resulting model is the same in terms of its predicted y values, but the coefficients and their interpretation are different.</p>
<p>THINK: Which two contrasts does this no-intercept model evaluate? (Hint: you can compare these numbers with the numbers produced by the previous model)</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">m.lin.noInt =<span class="st"> </span><span class="kw">lm</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition, <span class="dt">data =</span> data.md.red);</a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="kw">summary</span>(m.lin.noInt)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Effect_Size ~ 0 + Condition, data = data.md.red)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2080 -0.4233 -0.0426  0.3806  3.5744 
## 
## Coefficients:
##            Estimate Std. Error t value Pr(&gt;|t|)    
## ConditionW  0.47423    0.01471   32.23   &lt;2e-16 ***
## ConditionS  0.36225    0.01471   24.62   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7148 on 4718 degrees of freedom
## Multiple R-squared:  0.2585, Adjusted R-squared:  0.2582 
## F-statistic: 822.4 on 2 and 4718 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="was-it-worth-it-model-comparison" class="section level2">
<h2><span class="header-section-number">3.3</span> Was it worth it? Model comparison</h2>
<p>Does the model with Condition as a fixed effect explain more variance than the intercept-only model? To find out, we can compare the two models using the anova function. This function implements the <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test">likelihood ratio test</a>.</p>
<p>NOTE: the likelihood ratio (LR) test only works for nested models, i.e. models that have the same structure except that one model has some additional terms. The LR test would then tell us whether these terms are worth adding - in formal words, whether they significantly improve model fit. If you want to compare non-nested models, you can use other methods, such as <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">anova</span>(m.lin.noCond, m.lin)</a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Effect_Size ~ 1
## Model 2: Effect_Size ~ Condition
##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    
## 1   4719 2425.7                                 
## 2   4718 2410.9  1    14.796 28.954 7.77e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="random-effect-1-participant" class="section level2">
<h2><span class="header-section-number">3.4</span> Random effect #1: participant</h2>
<p>Now we’re ready to actually build a mixed effect model.</p>
<p>Our previous model assumed that all our participants had the same response to the S and W conditions - any variation was put in the error term. However, we know that some participants tend to have generally high responses and vice versa; thus, we might expect someone with a really high response to words to also have a high response to sentences. Can we incorporate that knowledge into the model?</p>
<p>We can! Let’s tell the model that the intercept varies across participants by adding an additional term to it:</p>
<p><span class="math display" id="eq:mixedmodel1">\[\begin{equation}
y = X * \vec{\beta} + \begin{bmatrix} 1\\...\\1  \end{bmatrix} * \vec{b} + \epsilon
\tag{3.5}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\vec{b}\)</span> specifies a participant-specific offset in the response strength - some will be above average, and some below average. This offset is not condition-specific (yet).</p>
<p>Why do we call it a random effect? Unlike Condition, each <span class="math inline">\(\vec{b}\)</span> value is participant-specific. Still, we could just add a bunch of columns to X for each participant and put 1’s against all trials completed by any particular participant. Then our participant-specific estimates would just be added to <span class="math inline">\(\vec{\beta}\)</span>.</p>
<p>In some cases, modeling participant-specific variation as a fixed effect might be a valid approach - for instance, if you have 3 participants and a ton of data for each. However, in our case, we have many participants but not much participant-specific data, so we would just get a lot of noisy effects. Moreover, all the-participant specific terms have something in common - they are not fully independent. For example, if your participant intercepts are 1, 1.1, 0.9, 1.2, you have a pretty strong hunch that the next one is not going to be 15. Mathematically, we can state that participant-specific intercepts come from a normal distribution where the mean is the group intercept (estimated as the first term in <span class="math inline">\(\vec{\beta}\)</span>), and the variance is a free parameter we need to estimate.</p>
<p>Thus, our <span class="math inline">\(\vec{b}\)</span> term is never evaluated directly! It is a random variable of the form</p>
<p><span class="math display">\[\begin{equation}
\vec{b} \sim \mathcal{N}(0,\sigma^{2})
\end{equation}\]</span></p>
<p>(the mean is 0 because this term is only estimating the participant-specific <em>offset</em> from the overall intercept)</p>
<p>With this ‘random effect’ trick, we achieve three goals:</p>
<ol style="list-style-type: decimal">
<li><p>We estimate the intercept effects across all participants by specifying just one parameter - <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
<li><p>We leverage the power of all the dataset, not just the participant-specific data. Thus if any one participant’s values are unusually high, it would not have a strong result on the model - the normal distribution will “regularize” the estimates for that participant.</p></li>
<li><p>We make <em>generalizable</em> inferences: instead of estimating the effects for each specific participant, we produce an estimate for the entire population.</p></li>
</ol>
<p>LET’S RUN OUR FIRST MIXED EFFECT MODEL!</p>
<p>The previous models did not have any random terms, and so we evaluated it using the ‘lm’ function. For mixed models, we use ‘lmer’.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># I want to evaluate the model by maximum likelihood, not restricted maximum likelihood (REML),</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="co"># so I&#39;m setting REML to FALSE</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">m.ri1 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>SubjectID), <span class="dt">data =</span> data.md.red,</a>
<a class="sourceLine" id="cb13-4" data-line-number="4">             <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb13-5" data-line-number="5"><span class="kw">summary</span>(m.ri1)</a></code></pre></div>
<pre><code>## Linear mixed model fit by maximum likelihood . t-tests use
##   Satterthwaite&#39;s method [lmerModLmerTest]
## Formula: Effect_Size ~ 0 + Condition + (1 | SubjectID)
##    Data: data.md.red
## 
##      AIC      BIC   logLik deviance df.resid 
##   7735.1   7761.0  -3863.6   7727.1     4716 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.4796 -0.5633 -0.0710  0.4803  5.7666 
## 
## Random effects:
##  Groups    Name        Variance Std.Dev.
##  SubjectID (Intercept) 0.2365   0.4864  
##  Residual              0.2758   0.5252  
## Number of obs: 4720, groups:  SubjectID, 115
## 
## Fixed effects:
##             Estimate Std. Error        df t value Pr(&gt;|t|)    
## ConditionW   0.47069    0.04663 121.41170  10.094  &lt; 2e-16 ***
## ConditionS   0.35872    0.04663 121.41170   7.693 4.24e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##            CndtnW
## ConditionS 0.946</code></pre>
<p>Note that the fixed effects structure is the same as before, but now we also have the “random effects” section, which tells us how much variance was explained by the intercept varying across participants and how much is left in the residuals.</p>
<p>The fixed effect estimates themselves have changed - adding the random effect changes the way the model is fitted to the data.</p>
<p>Does this random effect add to explained variance compared to the fixed-effect-only model?</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">m.noR =<span class="st"> </span>m.lin.noInt;</a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="kw">anova</span>(m.ri1, m.noR)</a></code></pre></div>
<pre><code>## Data: data.md.red
## Models:
## m.noR: Effect_Size ~ 0 + Condition
## m.ri1: Effect_Size ~ 0 + Condition + (1 | SubjectID)
##       Df     AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
## m.noR  3 10229.9 10249 -5111.9  10223.9                             
## m.ri1  4  7735.1  7761 -3863.6   7727.1 2496.7      1  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="random-intercept-2-experiment" class="section level2">
<h2><span class="header-section-number">3.5</span> Random intercept #2: experiment</h2>
<p>Another way to think about random effects is that they specify additional clusters in the data. For instance, most datapoints from participant A will be shifted above the mean, while most datapoints from participant B will be below the mean. Points A and points B will form two different clusters.</p>
<p>The dataset we’re using has another obvious source of clusters - it was collected across multiple experiments. Although we would hope that the data we record would be unaffected by the specific experimental conditions, in practice this is rarely the case. The data might be collected on different MRI scanners, during different seasons, the Sentence &amp; Word blocks might have had different lengths, etc. If you have all this information and you think that each variable might affect your results, then you can go ahead and enter these terms directly. However, we here will just use Experiment.</p>
<p>We assume that the mean responses to both conditions will vary across experiments (and, as before, across participants).</p>
<p>To save space, from now on we will only print out fixed and random effect estimates.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">m.ri2 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment), <span class="dt">data =</span> data.md.red,</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">             <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb17-3" data-line-number="3"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ri2))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##             Estimate Std. Error       df  t value     Pr(&gt;|t|)
## ConditionW 0.4563056 0.08056281 7.579648 5.663973 0.0005752957
## ConditionS 0.3443295 0.08056281 7.579648 4.274051 0.0030670741</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ri2)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev.
##  SubjectID  (Intercept) 0.44329 
##  Experiment (Intercept) 0.16633 
##  Residual               0.52429</code></pre>
<p>Previous model (m.ri1) reminder:
ConditionW 0.47423
ConditionS 0.36225</p>
<p>We see again that changing the random effect structure affects the fixed effects too.</p>
<p>Let’s compare our models to see if adding the experiment intercept actually improved model fit.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="kw">anova</span>(m.ri1, m.ri2)</a></code></pre></div>
<pre><code>## Data: data.md.red
## Models:
## m.ri1: Effect_Size ~ 0 + Condition + (1 | SubjectID)
## m.ri2: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment)
##       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
## m.ri1  4 7735.1 7761.0 -3863.6   7727.1                             
## m.ri2  5 7709.9 7742.2 -3850.0   7699.9 27.236      1  1.801e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>It did! We have a significant difference between the models, with model 2 having a higher log likelihood of the data.</p>
</div>
<div id="random-intercept-3-region" class="section level2">
<h2><span class="header-section-number">3.6</span> Random intercept #3: region</h2>
<p>Finally, we know that different brain regions have different levels of BOLD activity: regions close to large vessels will have strong responses to everything.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">m.ri3 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), </a>
<a class="sourceLine" id="cb23-2" data-line-number="2">             <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb23-3" data-line-number="3"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ri3))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##             Estimate Std. Error       df  t value     Pr(&gt;|t|)
## ConditionW 0.4566623 0.09492157 12.84781 4.810944 0.0003514695
## ConditionS 0.3446862 0.09492157 12.84781 3.631274 0.0030992971</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ri3)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev.
##  SubjectID  (Intercept) 0.44491 
##  Region     (Intercept) 0.21771 
##  Experiment (Intercept) 0.16917 
##  Residual               0.47755</code></pre>
<p>Previous model (m.ri2) reminder:
ConditionW 0.4563056
ConditionS 0.3443295</p>
<p>This time, the fixed effect estimates remained almost the same.</p>
<p>But did adding a random intercept by region improve model fit further? Yes, it did.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="kw">anova</span>(m.ri2, m.ri3)</a></code></pre></div>
<pre><code>## Data: data.md.red
## Models:
## m.ri2: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment)
## m.ri3: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + 
## m.ri3:     (1 | Region)
##       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
## m.ri2  5 7709.9 7742.2 -3850.0   7699.9                             
## m.ri3  6 6927.0 6965.8 -3457.5   6915.0 784.88      1  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="it-gets-better---random-slopes" class="section level2">
<h2><span class="header-section-number">3.7</span> It gets better - random slopes</h2>
<p>Ok, our model is getting pretty complicated. But as you were reading about all those random intercepts, you might have been wondering - what if it’s not just the mean activity that varies across participants/regions/experiments? What if participant A not only has stronger activity overall but also has stronger responses to Sentences specifically?</p>
<p>This kind of structure can be captured too. We call it a random <em>slope</em> because if we plot Condition on the x axis and Response on the y axis, the effect of interest will be the slope of the line going from the W value to the S value. High slope = steeper increase in activation as we go from W to S.</p>
<p>Remember that so far we have been specifying our random effects as (1 | RandomVar). What does this mean? It means, take the random effect on the right and estimate how it varies according to the fixed effects on the left. So far we’ve just had 1 on the left, which is our intercept. But now we also want to say that our random variables can vary by condition, so we add it to the left side too.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1">m.ris =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition<span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a></code></pre></div>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.0352573
## (tol = 0.002, component 1)</code></pre>
<p>Uh oh, what happened here? We get a warning saying that our model failed to converge. That’s not good - it means that our effect estimates might be imprecise.</p>
<p>What can we do to solve this? We could play around with different optimizers to find the one that works. But generally, the reason why our model here doesn’t converge is because it’s too complicated - we have too many variables and not enough data. So for now, let’s simplify and just add one random slope.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1">m.ris =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb31-2" data-line-number="2"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ris))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##             Estimate Std. Error       df  t value     Pr(&gt;|t|)
## ConditionW 0.4552242 0.09588469 13.57936 4.747622 0.0003385931
## ConditionS 0.3464231 0.09585435 13.57702 3.614057 0.0029489722</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.47456        
##             ConditionS  0.32323  -0.343
##  Region     (Intercept) 0.21798        
##  Experiment (Intercept) 0.16834        
##  Residual               0.44852</code></pre>
<p>Note that the model also estimates the covariance between the random effect’s intercept and slope (in essence, an extra variable to take care of). Here, the intercept and the slope are correlated: participants with higher overall responses will also have a higher S&gt;W value.</p>
<p>THINK: how can we modify equation <a href="building-your-first-mixed-model.html#eq:mixedmodel1">(3.5)</a> to incorporate the random slope by condition?</p>
<p>Let’s check: did adding a random slope help beyond random intercepts? It did.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="kw">anova</span>(m.ris, m.ri3)</a></code></pre></div>
<pre><code>## Data: data.md.red
## Models:
## m.ri3: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + 
## m.ri3:     (1 | Region)
## m.ris: Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 | 
## m.ris:     Experiment) + (1 | Region)
##       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
## m.ri3  6 6927.0 6965.8 -3457.5   6915.0                             
## m.ris  8 6567.3 6619.0 -3275.7   6551.3 363.69      2  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note that here I decided to only keep the random slope by participant, and it happened to actually be helpful. If possible, you should be more systematic when deciding on model structure. For details, see the next chapter.</p>
</div>
<div id="fixed-effects-can-be-more-complicated-too" class="section level2">
<h2><span class="header-section-number">3.8</span> Fixed effects can be more complicated too</h2>
<p>I will leave you with a final twist on our model.</p>
<p>So far we’ve only looked at Condition as our fixed effect: our goal was to get to random effects as soon as possible. But there is another important source of variation in our data - Hemisphere.</p>
<p>Since there are only two hemispheres, there is no reason to include it as a fixed effect (in this case, we’d have to imagine left and right hemispheres as samples from an infinite population of hemispheres…). So this is going to be our second fixed effect.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">m.ris2 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>Hemisphere <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb37-2" data-line-number="2"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ris2))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##               Estimate Std. Error       df   t value     Pr(&gt;|t|)
## ConditionW   0.5154497 0.10594839 18.13019  4.865102 0.0001220166
## ConditionS   0.4066486 0.10592197 18.12988  3.839134 0.0011885886
## HemisphereR -0.1204575 0.09446621 19.44581 -1.275139 0.2172850379</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris2)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.47453        
##             ConditionS  0.32325  -0.343
##  Region     (Intercept) 0.20921        
##  Experiment (Intercept) 0.16812        
##  Residual               0.44852</code></pre>
<p>Great, now we know that left and right hemisphere do not have significantly different overall activity levels<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. But we also want to know whether our Condition effects vary by Hemisphere. What do we need to include? An interaction.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1">m.ris3 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition<span class="op">*</span>Hemisphere <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ris3))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##                           Estimate Std. Error         df    t value
## ConditionW              0.49408827 0.10613083   18.29735  4.6554643
## ConditionS              0.42800976 0.10610243   18.29581  4.0339300
## HemisphereR            -0.07773581 0.09537081   20.18837 -0.8150901
## ConditionS:HemisphereR -0.08544343 0.02608228 4469.92124 -3.2759190
##                            Pr(&gt;|t|)
## ConditionW             0.0001890032
## ConditionS             0.0007568723
## HemisphereR            0.4245326475
## ConditionS:HemisphereR 0.0010612211</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris3)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.47461        
##             ConditionS  0.32334  -0.343
##  Region     (Intercept) 0.20923        
##  Experiment (Intercept) 0.16803        
##  Residual               0.44798</code></pre>
<p>Here, * is a shortcut meaning “estimate the main effects and the interaction”. Thus <code>Condition*Hemisphere</code> is equivalent to <code>Condition + Hemisphere + Condition:Hemisphere</code>, where <code>:</code> stands for the interaction itself.</p>
<p>And hey, looks like the Sentence responses in the right hemisphere MD regions are weaker than in the left.</p>
<p>Let’s do a final formal check to see whether adding the hemisphere terms was useful.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="kw">anova</span>(m.ris3, m.ris)</a></code></pre></div>
<pre><code>## Data: data.md.red
## Models:
## m.ris: Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 | 
## m.ris:     Experiment) + (1 | Region)
## m.ris3: Effect_Size ~ 0 + Condition * Hemisphere + (1 + Condition | SubjectID) + 
## m.ris3:     (1 | Experiment) + (1 | Region)
##        Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
## m.ris   8 6567.3 6619.0 -3275.7   6551.3                           
## m.ris3 10 6559.1 6623.7 -3269.5   6539.1 12.28      2   0.002155 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Yup, we explained additional variance.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">3.9</span> Summary</h2>
<ul>
<li>Fixed effects in a linear mixed effect model act just like regular regression terms.</li>
<li>For categorical variables, the first level of a fixed effect variable acts like a baseline (unless you set the intercept to 0).</li>
<li>Random effects are not simple regression terms; they are random variables. As such, you don’t estimate a value for each level (e.g. mean activation for each participant) but instead find the variance around the mean. For instance, you might estimate how much each participant’s data are likely to deviate from the mean value.</li>
<li>You can estimate random intercepts (how much does a random effect influence the baseline) and random slopes (how much does a random effect modulate a fixed effect)</li>
<li>You can decide whether adding a particular term improves model fit by conducting a likelihood ratio test.</li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>during word reading - see section <a href="the-devils-in-the-details.html#interactionsV2">4.2.2</a><a href="building-your-first-mixed-model.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="meet-the-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-devils-in-the-details.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
