<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 The devil’s in the details | Mixed Effect Models for fMRI</title>
  <meta name="description" content="4 The devil’s in the details | Mixed Effect Models for fMRI" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 The devil’s in the details | Mixed Effect Models for fMRI" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 The devil’s in the details | Mixed Effect Models for fMRI" />
  
  
  

<meta name="author" content="Compiled by Anna Ivanova based on materials by Idan Blank, Rachel Ryskin, and Cory Shain" />


<meta name="date" content="2020-12-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="building-your-first-mixed-model.html"/>
<link rel="next" href="meta-decisions.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION</a></li>
<li class="chapter" data-level="2" data-path="meet-the-data.html"><a href="meet-the-data.html"><i class="fa fa-check"></i><b>2</b> Meet the data</a><ul>
<li class="chapter" data-level="2.1" data-path="meet-the-data.html"><a href="meet-the-data.html#setting-up-the-environment"><i class="fa fa-check"></i><b>2.1</b> Setting up the environment</a></li>
<li class="chapter" data-level="2.2" data-path="meet-the-data.html"><a href="meet-the-data.html#the-data"><i class="fa fa-check"></i><b>2.2</b> The data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html"><i class="fa fa-check"></i><b>3</b> Building your first mixed model</a><ul>
<li class="chapter" data-level="3.1" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#building-from-the-ground-up"><i class="fa fa-check"></i><b>3.1</b> Building from the ground up</a></li>
<li class="chapter" data-level="3.2" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#a-model-with-1-fixed-effect"><i class="fa fa-check"></i><b>3.2</b> A model with 1 fixed effect</a></li>
<li class="chapter" data-level="3.3" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#was-it-worth-it-model-comparison"><i class="fa fa-check"></i><b>3.3</b> Was it worth it? Model comparison</a></li>
<li class="chapter" data-level="3.4" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#adding-random-intercepts"><i class="fa fa-check"></i><b>3.4</b> Adding random intercepts</a><ul>
<li class="chapter" data-level="3.4.1" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#random-intercept-1-participant"><i class="fa fa-check"></i><b>3.4.1</b> Random intercept #1: participant</a></li>
<li class="chapter" data-level="3.4.2" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#random-intercept-2-experiment"><i class="fa fa-check"></i><b>3.4.2</b> Random intercept #2: experiment</a></li>
<li class="chapter" data-level="3.4.3" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#random-intercept-3-region"><i class="fa fa-check"></i><b>3.4.3</b> Random intercept #3: region</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#randomslopes"><i class="fa fa-check"></i><b>3.5</b> It gets better - random slopes</a></li>
<li class="chapter" data-level="3.6" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#fixed-effects-can-be-more-complicated-too"><i class="fa fa-check"></i><b>3.6</b> Fixed effects can be more complicated too</a></li>
<li class="chapter" data-level="3.7" data-path="building-your-first-mixed-model.html"><a href="building-your-first-mixed-model.html#summary"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html"><i class="fa fa-check"></i><b>4</b> The devil’s in the details</a><ul>
<li class="chapter" data-level="4.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#testing-significance"><i class="fa fa-check"></i><b>4.1</b> Testing significance</a><ul>
<li class="chapter" data-level="4.1.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#method-1-read-the-p-values-directly-from-model-summary"><i class="fa fa-check"></i><b>4.1.1</b> Method 1: Read the p-values directly from model summary</a></li>
<li class="chapter" data-level="4.1.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#method-2-model-comparison"><i class="fa fa-check"></i><b>4.1.2</b> Method 2: Model comparison</a></li>
<li class="chapter" data-level="4.1.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#method-3-pairwise-tests"><i class="fa fa-check"></i><b>4.1.3</b> Method 3: pairwise tests</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#contrasts"><i class="fa fa-check"></i><b>4.2</b> Contrasts</a><ul>
<li class="chapter" data-level="4.2.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#intro-to-contrast-coding"><i class="fa fa-check"></i><b>4.2.1</b> Intro to contrast coding</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#interactionsV2"><i class="fa fa-check"></i><b>4.2.2</b> Applications to interactions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#convergence"><i class="fa fa-check"></i><b>4.3</b> Convergence issues</a><ul>
<li class="chapter" data-level="4.3.1" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-1-try-other-optimizers"><i class="fa fa-check"></i><b>4.3.1</b> Solution 1: try other optimizers</a></li>
<li class="chapter" data-level="4.3.2" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-2-brms"><i class="fa fa-check"></i><b>4.3.2</b> Solution 2: brms</a></li>
<li class="chapter" data-level="4.3.3" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#solution-3-reml"><i class="fa fa-check"></i><b>4.3.3</b> Solution 3: REML</a></li>
<li class="chapter" data-level="4.3.4" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#convergence-simplify"><i class="fa fa-check"></i><b>4.3.4</b> Solution 4: simplify your model</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-devils-in-the-details.html"><a href="the-devils-in-the-details.html#summary-1"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="meta-decisions.html"><a href="meta-decisions.html"><i class="fa fa-check"></i><b>5</b> Meta Decisions</a><ul>
<li class="chapter" data-level="5.1" data-path="meta-decisions.html"><a href="meta-decisions.html#brain-regions-fixed-or-random"><i class="fa fa-check"></i><b>5.1</b> Brain regions: fixed or random?</a><ul>
<li class="chapter" data-level="5.1.1" data-path="meta-decisions.html"><a href="meta-decisions.html#as-a-fixed-effect"><i class="fa fa-check"></i><b>5.1.1</b> As a fixed effect</a></li>
<li class="chapter" data-level="5.1.2" data-path="meta-decisions.html"><a href="meta-decisions.html#as-a-random-effect"><i class="fa fa-check"></i><b>5.1.2</b> As a random effect</a></li>
<li class="chapter" data-level="5.1.3" data-path="meta-decisions.html"><a href="meta-decisions.html#beyond-brain-region"><i class="fa fa-check"></i><b>5.1.3</b> Beyond brain region</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="meta-decisions.html"><a href="meta-decisions.html#model-complexity"><i class="fa fa-check"></i><b>5.2</b> Model complexity</a></li>
<li class="chapter" data-level="5.3" data-path="meta-decisions.html"><a href="meta-decisions.html#single-subject-mixed-analyses"><i class="fa fa-check"></i><b>5.3</b> Single-subject mixed analyses?</a></li>
<li class="chapter" data-level="5.4" data-path="meta-decisions.html"><a href="meta-decisions.html#conclusion"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mixed Effect Models for fMRI</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-devils-in-the-details" class="section level1">
<h1><span class="header-section-number">4</span> The devil’s in the details</h1>
<p>Now you know the basics of how to build a mixed effect model. But in some ways, your learning has just begun. Now you will face a million choices that you will have to make for each model you make. We discuss some of them in this chapter.</p>
<div id="testing-significance" class="section level2">
<h2><span class="header-section-number">4.1</span> Testing significance</h2>
<p>How do I know which terms in my model are significantly different from 0? We’ve already used some methods for estimating significance in the last chapter, but let’s be explicit here.</p>
<div id="method-1-read-the-p-values-directly-from-model-summary" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Method 1: Read the p-values directly from model summary</h3>
<p>Remember a term we had before, <code>ConditionS</code>? It is the estimate of the average <code>Effect_Size</code> for the Sentences &gt; Words contrast. How do we know whether this effect is significant?</p>
<p><code>lmerTest</code> allows us to just get a p value associated with each (thanks to this package, we get an extra column in your output for every fixed effect we estimate)</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1">m1 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Region), </a>
<a class="sourceLine" id="cb56-2" data-line-number="2">          <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb56-3" data-line-number="3"><span class="kw">summary</span>(m1)</a></code></pre></div>
<pre><code>## Linear mixed model fit by maximum likelihood . t-tests use
##   Satterthwaite&#39;s method [lmerModLmerTest]
## Formula: 
## Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) +  
##     (1 | Region)
##    Data: data.md.red
## 
##      AIC      BIC   logLik deviance df.resid 
##   6927.0   6965.8  -3457.5   6915.0     4714 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.2856 -0.5988 -0.0620  0.5180  5.9735 
## 
## Random effects:
##  Groups     Name        Variance Std.Dev.
##  SubjectID  (Intercept) 0.19795  0.4449  
##  Region     (Intercept) 0.04740  0.2177  
##  Experiment (Intercept) 0.02862  0.1692  
##  Residual               0.22806  0.4776  
## Number of obs: 4720, groups:  SubjectID, 115; Region, 20; Experiment, 6
## 
## Fixed effects:
##               Estimate Std. Error         df t value Pr(&gt;|t|)    
## (Intercept)    0.45666    0.09492   12.84781   4.811 0.000351 ***
## ConditionS    -0.11198    0.01390 4584.90113  -8.055 1.01e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##            (Intr)
## ConditionS -0.073</code></pre>
<p>Look at Fixed effects -&gt; ConditionS -&gt; value in the Pr(&gt;|t|) column is the p value of the “S” contrast, which in our case is Sentence&gt;Word.</p>
<p>Here’s how you can report this effect:</p>
<blockquote>
<p>The responses in the multiple demand (MD) network during sentence reading were lower than responses during word reading (beta=-0.11, SE=0.01, p&lt;.001).</p>
</blockquote>
<p>Note that lmerTest only estimates p values for fixed effects, so if you want to estimate the significance of a random effect, you have to use Method 2.</p>
</div>
<div id="method-2-model-comparison" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Method 2: Model comparison</h3>
<p>We’ve done this a lot in the previous chapter, but just to summarize:</p>
<ol style="list-style-type: decimal">
<li>Create a “null” model where the predictor of interest is missing</li>
</ol>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1">m1.null =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Experiment), </a>
<a class="sourceLine" id="cb58-2" data-line-number="2">          <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Use the likelihood ratio test (the <code>anova()</code> command) to find out if the model with more parameters fits the data better.</li>
</ol>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1"><span class="kw">anova</span>(m1.null, m1)</a></code></pre></div>
<pre><code>## Data: data.md.red
## Models:
## m1.null: Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment)
## m1: Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) + 
## m1:     (1 | Region)
##         Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
## m1.null  5 7709.9 7742.2 -3850.0   7699.9                             
## m1       6 6927.0 6965.8 -3457.5   6915.0 784.88      1  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This tells you that Region matters but it doesn’t tell you anything about which Region has higher or lower responses compared to the rest.</p>
<p>Here’s how you can describe this result:</p>
<blockquote>
<p>The overall response magnitude varied substantially by region (<span class="math inline">\(\sigma\)</span>=0.28). The likelihood ratio test showed that adding the region intercept term significantly improved model fit (<span class="math inline">\(X^2\)</span>=784.9, p&lt;.001).</p>
</blockquote>
</div>
<div id="method-3-pairwise-tests" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Method 3: pairwise tests</h3>
<p>If you just want the mean estimates for each condition and you want to compare them to 0, you can use <code>ls_means()</code> from <code>lmerTest</code></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="kw">ls_means</span>(m1)</a></code></pre></div>
<pre><code>## Least Squares Means table:
## 
##            Estimate Std. Error   df t value    lower    upper  Pr(&gt;|t|)
## ConditionW 0.456662   0.094922 12.8  4.8109 0.251350 0.661975 0.0003515
## ConditionS 0.344686   0.094922 12.8  3.6313 0.139373 0.549999 0.0030993
##               
## ConditionW ***
## ConditionS ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##   Confidence level: 95%
##   Degrees of freedom method: Satterthwaite</code></pre>
<p>Reporting:</p>
<blockquote>
<p>The MD network’s response to word reading was significantly above 0 (beta=0.41, SE=0.06, p&lt;.001), and so was the response to sentence reading (beta=0.27, SE = 0.09, p=.006).</p>
</blockquote>
<p>You can also use the same function to run a bunch of pairwise comparisons. In the current model, it was reasonably easy to read the effects right off the coefficients but when there are more than 2 levels per predictor this can get trickier so tools like this can be handy.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1"><span class="kw">ls_means</span>(m1, <span class="dt">pairwise =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Least Squares Means table:
## 
##                         Estimate Std. Error     df t value    lower
## ConditionW - ConditionS 0.111976   0.013902 4584.9  8.0546 0.084721
##                            upper  Pr(&gt;|t|)    
## ConditionW - ConditionS 0.139231 1.008e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##   Confidence level: 95%
##   Degrees of freedom method: Satterthwaite</code></pre>
<p>THINK: how many rows would we get if we had three conditions (say, Word, Sentence and Paragraph)? Four? Five? Would we get all these estimates after running the lmer model?</p>
<p>When models get more complicated you may need to switch to using <a href="https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html">emmeans</a>, which is another package with lots of great ways to probe models.</p>
<p>NOTE: when running follow-up analyses, don’t forget to account for multiple comparisons. Emmeans provides ways to do this automatically (by specifying the right parameters), but for lmerTest’s <code>ls_means</code> you’ll have to do it manually.</p>
<p>If you have multiple correlated predictors, things get harder. The main thing that can happen is that the estimates become unstable. In that case, it’s safer to do model comparison (method 2). You can almost always find out if an effect explains useful variance or not, but you can’t always tell what the direction or magnitude of the effect is.</p>
</div>
</div>
<div id="contrasts" class="section level2">
<h2><span class="header-section-number">4.2</span> Contrasts</h2>
<div id="intro-to-contrast-coding" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Intro to contrast coding</h3>
<p>Let’s take another look at the fixed effects in our model.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1">m1 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Region), </a>
<a class="sourceLine" id="cb65-2" data-line-number="2">          <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb65-3" data-line-number="3"><span class="kw">coef</span>(<span class="kw">summary</span>(m1)) </a></code></pre></div>
<pre><code>##               Estimate Std. Error         df   t value     Pr(&gt;|t|)
## (Intercept)  0.4566623 0.09492157   12.84781  4.810944 3.514693e-04
## ConditionS  -0.1119761 0.01390210 4584.90113 -8.054616 1.007830e-15</code></pre>
<p>As we’ve mentioned before, the first level (W) becomes the intercept, and all other levels are contrasted against it. But why is that?</p>
<p>Well let’s see how R is treating these levels. Ultimately, in order to fit the model, R has to somehow convert them into numbers. Using the <code>contrasts()</code> function, you can see what those numbers are.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="kw">contrasts</span>(data.md.red<span class="op">$</span>Condition)</a></code></pre></div>
<pre><code>##   S
## W 0
## S 1</code></pre>
<p>This is called <strong>dummy coding</strong> - but there’s nothing dumb about it! It’s just the default in R. Another term for it is “treatment coding”</p>
<p>But what if we don’t want to treat the word reading condition in this privileged way? Turns out, you can set the contrasts manually:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">data.md.red.sum =<span class="st"> </span><span class="kw">data.frame</span>(data.md.red)   <span class="co"># make a copy of the data</span></a>
<a class="sourceLine" id="cb69-2" data-line-number="2"><span class="kw">contrasts</span>(data.md.red.sum<span class="op">$</span>Condition) =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)    <span class="co"># change the contrast</span></a>
<a class="sourceLine" id="cb69-3" data-line-number="3"><span class="kw">colnames</span>(<span class="kw">attr</span>(data.md.red.sum<span class="op">$</span>Condition, <span class="st">&quot;contrasts&quot;</span>)) =<span class="st"> &quot;S&gt;W&quot;</span>     <span class="co"># name the contrast (optional)</span></a>
<a class="sourceLine" id="cb69-4" data-line-number="4"><span class="kw">contrasts</span>(data.md.red.sum<span class="op">$</span>Condition)     </a></code></pre></div>
<pre><code>##    S&gt;W
## W -0.5
## S  0.5</code></pre>
<p>Ok, looks confusing. What did we just do?</p>
<p>The difference between S and W remains 1. What has changed is where we place 0 (aka, the intercept). In dummy coding, 0 was aligned with the W condition. Now, it is <em>in between</em> W and S. Thus, our intercept will now reflect the average response across both W and S conditions.</p>
<p>This way of setting up contrasts is called <strong>sum coding</strong> (or deviation coding).</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1">m1.sum =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span>Region), </a>
<a class="sourceLine" id="cb71-2" data-line-number="2">          <span class="dt">data =</span> data.md.red.sum, <span class="dt">REML=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb71-3" data-line-number="3"><span class="kw">coef</span>(<span class="kw">summary</span>(m1.sum)) </a></code></pre></div>
<pre><code>##                Estimate Std. Error         df   t value     Pr(&gt;|t|)
## (Intercept)   0.4006743 0.09466671   12.71039  4.232473 1.027008e-03
## ConditionS&gt;W -0.1119761 0.01390210 4584.90113 -8.054616 1.007830e-15</code></pre>
<p>ConditionS&gt;W is still the difference between the 2 conditions.</p>
<p>Does it really matter where we place the intercept?</p>
<ol style="list-style-type: lower-alpha">
<li><p>It matters whenever you want to report the intercept value.</p></li>
<li><p>It matters even more when we want to interpret interactions.</p></li>
</ol>
</div>
<div id="interactionsV2" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Applications to interactions</h3>
<p>At the end of last chapter, we introduced a model with an interaction between 2 fixed effects - Condition and Hemisphere. Let’s take a closer look at what exactly it tells us about our data.</p>
<p>We’ll start with a quick plot to see what results we should be expecting</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="kw">ggplot</span>(data.md.red)<span class="op">+</span></a>
<a class="sourceLine" id="cb73-2" data-line-number="2"><span class="st">  </span><span class="kw">stat_summary</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Hemisphere, <span class="dt">y=</span>Effect_Size, <span class="dt">fill=</span>Condition), </a>
<a class="sourceLine" id="cb73-3" data-line-number="3">               <span class="dt">geom=</span><span class="st">&quot;col&quot;</span>, <span class="dt">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="dt">position=</span><span class="kw">position_dodge</span>(<span class="fl">0.8</span>))<span class="op">+</span></a>
<a class="sourceLine" id="cb73-4" data-line-number="4"><span class="st">  </span><span class="kw">stat_summary</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Hemisphere, <span class="dt">y=</span>Effect_Size, <span class="dt">group=</span>Condition), </a>
<a class="sourceLine" id="cb73-5" data-line-number="5">               <span class="dt">geom=</span><span class="st">&#39;errorbar&#39;</span>, <span class="dt">fun.data=</span><span class="st">&#39;mean_se&#39;</span>, <span class="dt">position=</span><span class="kw">position_dodge</span>(<span class="fl">0.8</span>), <span class="dt">width=</span><span class="fl">0.2</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-197-1.png" width="672" /></p>
<p>Based on this plot, we would expect to see the main effect of condition (W&gt;S), maybe the main effect of hemisphere (L&gt;R), and likely an interaction between hemisphere and condition (the W&gt;S effect is larger in RH). Was that what we observed?</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1">m2 =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span>Condition<span class="op">*</span>Hemisphere <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb74-2" data-line-number="2"><span class="kw">coef</span>(<span class="kw">summary</span>(m2))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##                           Estimate Std. Error         df    t value
## (Intercept)             0.49408827 0.10613083   18.29735  4.6554643
## ConditionS             -0.06607851 0.03537294  153.88236 -1.8680525
## HemisphereR            -0.07773581 0.09537081   20.18837 -0.8150901
## ConditionS:HemisphereR -0.08544343 0.02608228 4469.92124 -3.2759190
##                            Pr(&gt;|t|)
## (Intercept)            0.0001890031
## ConditionS             0.0636561836
## HemisphereR            0.4245326473
## ConditionS:HemisphereR 0.0010612211</code></pre>
<p>The interaction between condition and hemisphere is indeed significant (beta=-0.08, SE=0.03, p=.001), but the effect of condition is not (beta=-0.07, SE=0.04, p=.064), and neither is the effect of hemisphere (beta=-0.08, SE=0.10, p=.424). Why is that?</p>
<p>Our intuition doesn’t align with the model output because the hemisphere and condition effects here do not actually reflect the main effect of each factor. The reason why is because each fixed effect is evaluated with respect to the <em>intercept</em> of the other one.</p>
<p>Remember, this is the dataset where we’re using the default dummy coding, so the intercept for condition is words and the intercept for hemisphere is left. As a result,</p>
<ul>
<li><p>under ConditionW we have the <strong>simple effect</strong> of Condition: Words&gt;Sentences but only for the left hemisphere. And it’s indeed pretty small.</p></li>
<li><p>under HemisphereR we have the <strong>simple effect</strong> of Hemisphere: Right&gt;Left but only for words. Looking at the plot, we can verify that the hemisphere difference for words is indeed ~0.08 (as predicted by our beta value).</p></li>
</ul>
<p>In order to estimate the <strong>main effects</strong>, we want to use sum coding. Then, the intercept will reflect the average response for each factor across levels.</p>
<p>We’ve already done this for condition:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1"><span class="kw">contrasts</span>(data.md.red.sum<span class="op">$</span>Condition)</a></code></pre></div>
<pre><code>##    S&gt;W
## W -0.5
## S  0.5</code></pre>
<p>Let’s do it for hemisphere too:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="kw">contrasts</span>(data.md.red.sum<span class="op">$</span>Hemisphere) =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb78-2" data-line-number="2"><span class="kw">colnames</span>(<span class="kw">attr</span>(data.md.red.sum<span class="op">$</span>Hemisphere, <span class="st">&quot;contrasts&quot;</span>)) =<span class="st"> &quot;R&gt;L&quot;</span> </a>
<a class="sourceLine" id="cb78-3" data-line-number="3"><span class="kw">contrasts</span>(data.md.red.sum<span class="op">$</span>Hemisphere)</a></code></pre></div>
<pre><code>##    R&gt;L
## L -0.5
## R  0.5</code></pre>
<p>Now the effect of hemisphere will be estimated with respect to the <em>average</em> response for words and sentences, and the effect of condition will be estimated with respect to the <em>average</em> response across hemispheres.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1">m2.sum =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span>Condition<span class="op">*</span>Hemisphere <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red.sum, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a>
<a class="sourceLine" id="cb80-2" data-line-number="2"><span class="kw">coef</span>(<span class="kw">summary</span>(m2.sum))    </a></code></pre></div>
<pre><code>##                               Estimate Std. Error         df   t value
## (Intercept)                 0.40082257 0.09337878   12.41336  4.292438
## ConditionS&gt;W               -0.10880025 0.03288109  114.96989 -3.308900
## HemisphereR&gt;L              -0.12045752 0.09446968   19.44370 -1.275092
## ConditionS&gt;W:HemisphereR&gt;L -0.08544343 0.02608229 4469.91856 -3.275917
##                                Pr(&gt;|t|)
## (Intercept)                0.0009697075
## ConditionS&gt;W               0.0012510800
## HemisphereR&gt;L              0.2173028893
## ConditionS&gt;W:HemisphereR&gt;L 0.0010612282</code></pre>
<p>Notice that the interaction remained the same but the simple effects changed. The effect of condition is now significant (beta=-0.11, SE=0.03, p=.001); the effect of hemisphere is not significant (beta=-0.12, SE=0.09, p=.217), but its estimate is larger and actually reflects the mean difference between L and R hemispheres.</p>
<p>THINK: What does the intercept term reflect in <code>m2</code> and in <code>m2.sum</code>?</p>
<p>Of course, we don’t always want to transform everything to sum coding. It might be totally reasonable to estimate the effect of hemisphere for words rather than for the word/sentence average. You just want to be clear about how you’re coding your contrasts because it will have a big impact on how to interpret the results.</p>
</div>
</div>
<div id="convergence" class="section level2">
<h2><span class="header-section-number">4.3</span> Convergence issues</h2>
<p>Remember we got a warning in section <a href="building-your-first-mixed-model.html#randomslopes">3.5</a> saying that our model doesn’t converge? We’ll repeat that example here:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1">m.ris =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition<span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>);</a></code></pre></div>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.0352573
## (tol = 0.002, component 1)</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ris))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##             Estimate Std. Error        df  t value     Pr(&gt;|t|)
## ConditionW 0.4547675 0.08210915  9.847007 5.538573 0.0002621747
## ConditionS 0.3378952 0.10445966 13.926610 3.234696 0.0060276813</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.479932       
##             ConditionS  0.318837 -0.364
##  Region     (Intercept) 0.184415       
##             ConditionS  0.080952 0.795 
##  Experiment (Intercept) 0.132303       
##             ConditionS  0.059135 0.818 
##  Residual               0.446580</code></pre>
<p>Let’s take a look at the strategies we can use to resolve this issue.</p>
<div id="solution-1-try-other-optimizers" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Solution 1: try other optimizers</h3>
<p>We can use a different optimizer and/or explicitly specify the stopping criterion.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">m.ris.bobyqa =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition<span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb88-2" data-line-number="2">                <span class="dt">control =</span> <span class="kw">lmerControl</span>(<span class="dt">optimizer =</span> <span class="st">&quot;bobyqa&quot;</span>, <span class="dt">optCtrl=</span><span class="kw">list</span>(<span class="dt">maxfun=</span><span class="fl">2e5</span>)))</a>
<a class="sourceLine" id="cb88-3" data-line-number="3"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ris.bobyqa))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##             Estimate Std. Error        df  t value     Pr(&gt;|t|)
## ConditionW 0.4544214 0.08190374  8.989032 5.548237 0.0003587971
## ConditionS 0.3378332 0.10455535 13.765356 3.231143 0.0061498913</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris.bobyqa)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.480476       
##             ConditionS  0.319156 -0.366
##  Region     (Intercept) 0.184321       
##             ConditionS  0.080824 0.795 
##  Experiment (Intercept) 0.131508       
##             ConditionS  0.058361 0.863 
##  Residual               0.446573</code></pre>
<p>It worked! No more convergence issues.</p>
<p>What if bobyqa didn’t converge? Instead of just specifying a single optimizer, you can use the function <code>allFit</code> which will run all the optimizers for you, and hopefully at least some of them fit. See <a href="https://rdrr.io/cran/lme4/man/allFit.html">lme4 documentation</a> for more details.</p>
<p><code>allFit</code> can also be used to determine whether you can trust your estimates. For this, you can fit the model with multiple optimizers, look at the fixed effects and determine whether the estimates across all optimizers are the same (out to ~4 sig digits). If they are, then it’s probably fine to use the model estimates.</p>
</div>
<div id="solution-2-brms" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Solution 2: brms</h3>
<p><a href="https://paul-buerkner.github.io/brms/" class="uri">https://paul-buerkner.github.io/brms/</a></p>
<p>This is a package for Bayesian modeling; it is based on the probabilistic programming language called Stan. The syntax for specifying the model is very similar to lme4, but switching from a frequentist to a Bayesian approach to modeling might require some extra work. These models might also take a while to run.</p>
</div>
<div id="solution-3-reml" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Solution 3: REML</h3>
<p>In all the examples above, we fit our model using maximum likelihood. The default for <code>lmer</code> is actually <a href="https://en.wikipedia.org/wiki/Restricted_maximum_likelihood">restricted maximum likelihood</a> (REML). Because it fits the data in stages, in some cases it might make it easier for the model to converge.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1">m.ris.reml =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition<span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb92-2" data-line-number="2"></a>
<a class="sourceLine" id="cb92-3" data-line-number="3"><span class="kw">summary</span>(m.ris.reml)</a></code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 +  
##     Condition | Experiment) + (1 + Condition | Region)
##    Data: data.md.red
## 
## REML criterion at convergence: 6527
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.6493 -0.5807 -0.0683  0.5089  5.9141 
## 
## Random effects:
##  Groups     Name        Variance Std.Dev. Corr 
##  SubjectID  (Intercept) 0.229264 0.47882       
##             ConditionS  0.101223 0.31816  -0.36
##  Region     (Intercept) 0.034497 0.18573       
##             ConditionS  0.006672 0.08168  0.79 
##  Experiment (Intercept) 0.023741 0.15408       
##             ConditionS  0.005286 0.07270  0.47 
##  Residual               0.199429 0.44657       
## Number of obs: 4720, groups:  SubjectID, 115; Region, 20; Experiment, 6
## 
## Fixed effects:
##            Estimate Std. Error       df t value Pr(&gt;|t|)    
## ConditionW  0.45783    0.08837  8.54662   5.181 0.000683 ***
## ConditionS  0.33841    0.10926 11.06806   3.097 0.010086 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##            CndtnW
## ConditionS 0.904</code></pre>
<p>This method worked too. Note, however, that if you then try to use the <code>anova()</code> function on the model, it will automatically refit using maximum likelihood - which brings us back to square one.</p>
</div>
<div id="convergence-simplify" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Solution 4: simplify your model</h3>
<p>The convergence issues can arise if your model is too complicated and/or if there isn’t enough data to estimate all the parameters.</p>
<p>THINK: how many parameters does <code>m.ris</code> need to estimate? (Hint: don’t forget about the correlations between random effects)</p>
<p>There is no one answer to how many samples are sufficient, but if it’s 2 per parameter, it’s too low (e.g. we’re estimating the participant by condition slope, and we only have estimates for 2 ROIs, it’s hard to decide how much variance should go into ROI terms and how much should be assigned to the participant x condition slope). Note that it’s not about the number of datapoints but rather the number of groupings.</p>
<p>In some cases, you will get a <code>singular fit</code> warning. We can also check for it explicitly:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1"><span class="kw">isSingular</span>(m.ris)</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>A singular model means that one of the random effects (or a combination of random effects) is zero. While mathematically such a model is well-defined, in practice this model often means that the model overfits: there is not enough power to distinguish</p>
<p>All in all, if you suspect that your model is at risk of overfitting the data, you should consider simplifying your model.</p>
<p>A general recommendation is to first remove random effects which account for least variance.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris)</a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.479932       
##             ConditionS  0.318837 -0.364
##  Region     (Intercept) 0.184415       
##             ConditionS  0.080952 0.795 
##  Experiment (Intercept) 0.132303       
##             ConditionS  0.059135 0.818 
##  Residual               0.446580</code></pre>
<p>(Admittedly, this step is a bit weird since the reason we’re simplifying the model is because we don’t trust these estimates to begin with. You can also try estimating these random effects individually in simpler models &amp; then comparing the magnitudes.)</p>
<p>Our lowest random effect is the Condition x Experiment slope, so let’s try removing that:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1">m.ris.red =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.00201745
## (tol = 0.002, component 1)</code></pre>
<p>Okay that didn’t do it… Let’s try one more - the variance estimate for the Condition x Region slope is also pretty low.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1">m.ris.red =<span class="st"> </span><span class="kw">lmer</span>(Effect_Size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Condition <span class="op">|</span><span class="st"> </span>SubjectID) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Experiment) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Region), <span class="dt">data =</span> data.md.red, <span class="dt">REML=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb100-2" data-line-number="2"></a>
<a class="sourceLine" id="cb100-3" data-line-number="3"><span class="kw">coef</span>(<span class="kw">summary</span>(m.ris.red))    <span class="co"># fixed</span></a></code></pre></div>
<pre><code>##             Estimate Std. Error       df  t value     Pr(&gt;|t|)
## ConditionW 0.4552242 0.09588469 13.57936 4.747622 0.0003385931
## ConditionS 0.3464231 0.09585435 13.57702 3.614057 0.0029489722</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1"><span class="kw">VarCorr</span>(m.ris.red)          <span class="co"># random</span></a></code></pre></div>
<pre><code>##  Groups     Name        Std.Dev. Corr  
##  SubjectID  (Intercept) 0.47456        
##             ConditionS  0.32323  -0.343
##  Region     (Intercept) 0.21798        
##  Experiment (Intercept) 0.16834        
##  Residual               0.44852</code></pre>
<p>Alright, now our model finally converges.</p>
<p>In short, here is what you should do when encountering convergence issues:</p>
<ol style="list-style-type: decimal">
<li><p>Make sure your model structure is justified (more on that in the next chapter).</p></li>
<li><p>Try to make the model converge by tinkering with the optimization procedure.</p></li>
</ol>
</div>
</div>
<div id="summary-1" class="section level2">
<h2><span class="header-section-number">4.4</span> Summary</h2>
<ul>
<li><p>You can estimate the significance of fixed effects using the lmerTest package; besides the p value, we also recommend reporting the effect size and the standard error of the estimate.</p></li>
<li><p>You can estimate the significance of a random effect by performing the likelihood ratio test; note, however, that this test does not allow you to estimate the direction or the magnitude of the effect.</p></li>
<li><p>Be mindful of the contrasts you’re setting for your categorical variables (explicitly or implicitly).</p></li>
<li><p>When running into convergence issues, make sure your model structure makes sense given your dataset and then try one of the technical solutions, e.g. a different optimizer.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="building-your-first-mixed-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="meta-decisions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
