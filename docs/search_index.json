[["index.html", "Mixed Effect Models for fMRI 1 INTRODUCTION", " Mixed Effect Models for fMRI Compiled by Anna Ivanova based on materials by Idan Blank, Rachel Ryskin, and Cory Shain 2020-12-04 1 INTRODUCTION This is a tutorial on how to conduct group-level statistical analyses of fMRI data using mixed effect linear models in R. It arose out of an extended Evlab meeting and reflects our practices at the time. We expect to develop and improve upon them as time goes by. This tutorial assumes that you have already conducted subject-level analyses and extracted average response values for each brain region of interest (ROI). It assumes no previous knowledge of R and only a bit of stats (e.g. we will remind you the basics of linear regression but will assume that you know what p-values are for). Our goal is to: Explain the basic logic behind linear mixed effect models. Show how to run them in R using the lme4 package. Discuss how to interpret the results you get. Highlight the major decision points you as a modeler will have to make along the way. "],["meet-the-data.html", "2 Meet the data 2.1 Setting up the environment 2.2 The data", " 2 Meet the data 2.1 Setting up the environment We will need the following packages: tidyverse: this is actually a collection of packages that we will use for dataframe manipulations and plotting the data lme4: this is the package that provides us with tools for running the mixed effects model (such as the lmer function) lmerTest: this is an addon to the lme4 package that will estimate significance (p values) for each term # BEGINNNER TIP: before loading a package for the first time, you need to install it # using the command install.packages(&quot;myPackageNameInQuotes&quot;) library(tidyverse) library(lme4) library(lmerTest) 2.2 The data We will use data from the paper by Diachek et al (2020): “The domain-general multiple demand (MD) network does not support core aspects of language comprehension: a large-scale fMRI investigation”. In this work, the authors examined responses within 2 networks of interest - multiple demand and language - to sentence reading &amp; word reading conditions across many different experiments. Each network contains multiple regions of interest (ROIs), 20 for MD and 6 for language; the MD regions are located in both hemispheres, whereas the language ROIs are only located in the left hemisphere. The data can be downloaded from OSF; a csv version we use in this tutorial is available in the tutorial’s Github repo. Let’s load and examine the data. In this tutorial, we will only be looking at the MD system. # load data data = read.csv(&#39;data/Diachek2020.csv&#39;, header=TRUE) data = subset(data, (System==&quot;MD&quot;)) print(str(data)) ## &#39;data.frame&#39;: 20960 obs. of 9 variables: ## $ Experiment : Factor w/ 30 levels &quot;Experiment1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ System : Factor w/ 2 levels &quot;language&quot;,&quot;MD&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Region : Factor w/ 30 levels &quot;LAntTemp&quot;,&quot;LH_antParietal&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ SubjectID : Factor w/ 679 levels &quot;007_KAN_parametric_07&quot;,..: 2 10 11 12 56 74 96 107 111 169 ... ## $ Condition : Factor w/ 2 levels &quot;S&quot;,&quot;W&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Hemisphere : Factor w/ 2 levels &quot;L&quot;,&quot;R&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Effect_Size: num 0.602 -0.303 -0.369 0.361 -0.194 ... ## $ Modality : Factor w/ 2 levels &quot;auditory&quot;,&quot;visual&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Task : Factor w/ 2 levels &quot;passive&quot;,&quot;task&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## NULL You want to make sure that all your categorical variables are specified as factors. Generally, R will default to treating numbers as type “numeric” and strings as factors, but it might help to be explicit: # COMMON MISTAKE: if you use numeric IDs (for participants, experiments, etc), they will be treated as numeric # (i.e. experiment 2 &gt; experiment 1). This is usually not what we want, so we can explicitly tell R that we want to treat a certain variable as a factor. data$Experiment = factor(data$Experiment) You can also use the factor command to specify the order of the different values (“levels”) that a given variable can take. If you don’t specify the order, the levels will be ordered alphabetically. data$Condition = factor(data$Condition, levels=c(&#39;W&#39;, &#39;S&#39;)) Let’s take a look at our data! ggplot(data)+ stat_summary(aes(x=Condition, y=Effect_Size), fun.y=&quot;mean&quot;, geom=&quot;col&quot;)+ facet_wrap(~Region, ncol = 5)+ labs(title=&quot;By Region&quot;)+ theme(legend.position = &quot;none&quot;, plot.title=element_text(face=&quot;bold&quot;)) ggplot(data)+ stat_summary(aes(x=Condition, y=Effect_Size), fun.y=&quot;mean&quot;, geom=&quot;col&quot;)+ facet_wrap(~Experiment, ncol = 6)+ labs(title=&quot;By Experiment&quot;)+ theme(legend.position = &quot;none&quot;, plot.title=element_text(face=&quot;bold&quot;)) We see that the responses vary quite a lot by brain region and experiment. Moreover, some experiments only include a sentence (S) or word (W) condition but not both. For simplicity, let’s only include a subset of experiments (19-24) for (ii in 19:24) { x = subset(data, Experiment == paste(&quot;Experiment&quot;, as.character(ii), sep=&quot;&quot;)) if (ii==19){ data.md.red = x } else { data.md.red = rbind(data.md.red,x) } } summary(data.md.red) ## Experiment System Region ## Experiment23:1320 language: 0 LH_antParietal : 236 ## Experiment24: 840 MD :4720 LH_insula : 236 ## Experiment19: 680 LH_medialFrontal: 236 ## Experiment20: 640 LH_midFrontal : 236 ## Experiment21: 640 LH_midFrontalOrb: 236 ## Experiment22: 600 LH_midParietal : 236 ## (Other) : 0 (Other) :3304 ## SubjectID Condition Hemisphere Effect_Size ## 365_FED_20170510a_3T2: 80 W:2360 L:2360 Min. :-2.845794 ## 498_FED_20170510b_3T2: 80 S:2360 R:2360 1st Qu.:-0.000482 ## 541_FED_20170523d_3T2: 80 Median : 0.371629 ## 007_KAN_parametric_07: 40 Mean : 0.418242 ## 018_FED_20151202b_3T1: 40 3rd Qu.: 0.790246 ## 018_FED_20151203a_3T1: 40 Max. : 4.048679 ## (Other) :4360 ## Modality Task ## auditory: 0 passive:1480 ## visual :4720 task :3240 ## ## ## ## ## Now we’re ready to analyze our data! "],["building-your-first-mixed-model.html", "3 Building your first mixed model 3.1 Building from the ground up 3.2 A model with 1 fixed effect 3.3 Was it worth it? Model comparison 3.4 Adding random intercepts 3.5 It gets better - random slopes 3.6 Fixed effects can be more complicated too 3.7 Summary", " 3 Building your first mixed model 3.1 Building from the ground up Let’s start with the simplest linear model: only an intercept. This looks like: \\[\\begin{equation} \\vec{y} = \\begin{bmatrix} 1\\\\...\\\\1 \\end{bmatrix} * \\beta_0 + \\epsilon \\tag{3.1} \\end{equation}\\] wherу \\(\\vec{y}\\) is a vector with the measurements we want to model (in our example, Effect_Size), \\(\\beta_0\\) is the baseline response, and \\(\\epsilon\\) is residual error. The length of the ones vector is the same as the number of observations in \\(\\vec{y}\\). Essentially, this model is predicting the same number for every single measurement. In R syntax, we can say that \\(\\vec{y}\\) is proportional to some value (and estimate the coefficients that make this proportion work out). In this case, we assume that each value in \\(\\vec{y}\\) is a constant and therefore proportional to 1: \\[\\begin{equation} y \\propto 1 \\tag{3.2} \\end{equation}\\] Here’s what it looks like in the code: # specify the model m.lin.noCond = lm(Effect_Size ~ 1, data = data.md.red) # show the result summary(m.lin.noCond) ## ## Call: ## lm(formula = Effect_Size ~ 1, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2640 -0.4187 -0.0466 0.3720 3.6304 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.41824 0.01044 40.08 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.717 on 4719 degrees of freedom 3.2 A model with 1 fixed effect Let’s add a single fixed predictor. Now our effect size \\(\\vec{y}\\) depends not only on the baseline response, but also on the experimental condition. The general formula for such a design looks like this: \\[\\begin{equation} y = X * \\vec{\\beta} + \\epsilon \\tag{3.3} \\end{equation}\\] What’s X? X is the design matrix; it specifies our fixed effects for this model (fixed effects are like regular regression terms - in fact, if you’ve seen matrix-form regression equations before, this should all be familiar). The size of X will depend on the number of conditions. How many conditions do we have here? str(unique(data$Condition)) ## Factor w/ 2 levels &quot;W&quot;,&quot;S&quot;: 2 1 We have 2 conditions, words (W) and sentences (S). So our design matrix will look something like the following: \\[\\begin{equation} X = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ ...\\\\ 1 &amp; 1 \\end{bmatrix} \\tag{3.4} \\end{equation}\\] where the first column is our intercept (“baseline” neural activity) and the second column has 1 whenever that row comes from the Sentence condition and 0 otherwise. Where is the Word condition? Remember that we said earlier that the first level of the factor variable gets treated as the baseline. So here, the response to words is our baseline, and the response to sentences is the stuff you would get “on top” of the word-induced activation - in other words, the magnitude of the S&gt;N contrast. The intercept, in turn, would give you the average magnitude of the W&gt;baseline contrast. This might sound a bit counterintuitive, but imagine if we tried to specify the intercept, word and sentence as 3 separate effects to estimate. Then we could still compare sentences to words, but would have trouble distinguishing the words from the intercept. If our observed response to words is 1.2, does the intercept contribute 0 and words 1.2? intercept 1.2 and words 0? intercept 0.6 and words 0.6? There is an infinite number of combinations. (Mathematically, we can state that the matrix (3.4) would be rank-deficient if we added the predictor variable for S, since it’s a direct complement of W. Thus, there would be an infinite number of coefficients \\(\\beta\\) for solving equation (3.3)) Bottom line: if you have n levels, you can estimate the maximum of n effects. Ok, with all those preliminaries behind us, let’s finally run the model! m.lin = lm(Effect_Size ~ Condition, data = data.md.red) summary(m.lin) ## ## Call: ## lm(formula = Effect_Size ~ Condition, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2080 -0.4233 -0.0426 0.3806 3.5744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.47423 0.01471 32.228 &lt; 2e-16 *** ## ConditionS -0.11198 0.02081 -5.381 7.77e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7148 on 4718 degrees of freedom ## Multiple R-squared: 0.0061, Adjusted R-squared: 0.005889 ## F-statistic: 28.95 on 1 and 4718 DF, p-value: 7.77e-08 We do not need to explicitly specify an intercept: a model with +1 is the same as a model without it m.lin.int = lm(Effect_Size ~ 1 + Condition, data = data.md.red); summary(m.lin.int) ## ## Call: ## lm(formula = Effect_Size ~ 1 + Condition, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2080 -0.4233 -0.0426 0.3806 3.5744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.47423 0.01471 32.228 &lt; 2e-16 *** ## ConditionS -0.11198 0.02081 -5.381 7.77e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7148 on 4718 degrees of freedom ## Multiple R-squared: 0.0061, Adjusted R-squared: 0.005889 ## F-statistic: 28.95 on 1 and 4718 DF, p-value: 7.77e-08 We can explicitly supress the intercept. The resulting model is the same in terms of its predicted y values, but the coefficients and their interpretation are different. THINK: Which two contrasts does this no-intercept model evaluate? (Hint: you can compare these numbers with the numbers produced by the previous model) m.lin.noInt = lm(Effect_Size ~ 0 + Condition, data = data.md.red); summary(m.lin.noInt) ## ## Call: ## lm(formula = Effect_Size ~ 0 + Condition, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2080 -0.4233 -0.0426 0.3806 3.5744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## ConditionW 0.47423 0.01471 32.23 &lt;2e-16 *** ## ConditionS 0.36225 0.01471 24.62 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7148 on 4718 degrees of freedom ## Multiple R-squared: 0.2585, Adjusted R-squared: 0.2582 ## F-statistic: 822.4 on 2 and 4718 DF, p-value: &lt; 2.2e-16 3.3 Was it worth it? Model comparison Does the model with Condition as a fixed effect explain more variance than the intercept-only model? To find out, we can compare the two models using the anova function. This function implements the likelihood ratio test. NOTE: the likelihood ratio (LR) test only works for nested models, i.e. models that have the same structure except that one model has some additional terms. The LR test would then tell us whether these terms are worth adding - in formal words, whether they significantly improve model fit. If you want to compare non-nested models, you can use other methods, such as AIC. anova(m.lin.noCond, m.lin) ## Analysis of Variance Table ## ## Model 1: Effect_Size ~ 1 ## Model 2: Effect_Size ~ Condition ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 4719 2425.7 ## 2 4718 2410.9 1 14.796 28.954 7.77e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.4 Adding random intercepts 3.4.1 Random intercept #1: participant Now we’re ready to actually build a mixed effect model. Our previous model assumed that all our participants had the same response to the S and W conditions - any variation was put in the error term. However, we know that some participants tend to have generally high responses and vice versa; thus, we might expect someone with a really high response to words to also have a high response to sentences. Can we incorporate that knowledge into the model? We can! Let’s tell the model that the intercept varies across participants by adding an additional term to it: \\[\\begin{equation} y = X * \\vec{\\beta} + \\begin{bmatrix} 1\\\\...\\\\1 \\end{bmatrix} * \\vec{b} + \\epsilon \\tag{3.5} \\end{equation}\\] Here, \\(\\vec{b}\\) specifies a participant-specific offset in the response strength - some will be above average, and some below average. This offset is not condition-specific (yet). Why do we call it a random effect? Unlike Condition, each \\(\\vec{b}\\) value is participant-specific. Still, we could just add a bunch of columns to X for each participant and put 1’s against all trials completed by any particular participant. Then our participant-specific estimates would just be added to \\(\\vec{\\beta}\\). In some cases, modeling participant-specific variation as a fixed effect might be a valid approach - for instance, if you have 3 participants and a ton of data for each. However, in our case, we have many participants but not much participant-specific data, so we would just get a lot of noisy effects. Moreover, all the-participant specific terms have something in common - they are not fully independent. For example, if your participant intercepts are 1, 1.1, 0.9, 1.2, you have a pretty strong hunch that the next one is not going to be 15. Mathematically, we can state that participant-specific intercepts come from a normal distribution where the mean is the group intercept (estimated as the first term in \\(\\vec{\\beta}\\)), and the variance is a free parameter we need to estimate. Thus, our \\(\\vec{b}\\) term is never evaluated directly! It is a random variable of the form \\[\\begin{equation} \\vec{b} \\sim \\mathcal{N}(0,\\sigma^{2}) \\end{equation}\\] (the mean is 0 because this term is only estimating the participant-specific offset from the overall intercept) With this ‘random effect’ trick, we achieve three goals: We estimate the intercept effects across all participants by specifying just one parameter - \\(\\sigma^{2}\\). We leverage the power of all the dataset, not just the participant-specific data. Thus if any one participant’s values are unusually high, it would not have a strong result on the model - the normal distribution will “regularize” the estimates for that participant. We make generalizable inferences: instead of estimating the effects for each specific participant, we produce an estimate for the entire population. LET’S RUN OUR FIRST MIXED EFFECT MODEL! The previous models did not have any random terms, and so we evaluated it using the ‘lm’ function. For mixed models, we use ‘lmer’. # I want to evaluate the model by maximum likelihood, not restricted maximum likelihood (REML), # so I&#39;m setting REML to FALSE m.ri1 = lmer(Effect_Size ~ Condition + (1 | SubjectID), data = data.md.red, REML=FALSE); summary(m.ri1) ## Linear mixed model fit by maximum likelihood . t-tests use ## Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: Effect_Size ~ Condition + (1 | SubjectID) ## Data: data.md.red ## ## AIC BIC logLik deviance df.resid ## 7735.1 7761.0 -3863.6 7727.1 4716 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.4796 -0.5633 -0.0710 0.4803 5.7666 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SubjectID (Intercept) 0.2365 0.4864 ## Residual 0.2758 0.5252 ## Number of obs: 4720, groups: SubjectID, 115 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.47069 0.04663 121.41170 10.094 &lt; 2e-16 *** ## ConditionS -0.11198 0.01529 4604.97757 -7.324 2.82e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ConditionS -0.164 Note that the fixed effects structure is the same as before, but now we also have the “random effects” section, which tells us how much variance was explained by the intercept varying across participants and how much is left in the residuals. The fixed effect estimates themselves have changed - adding the random effect changes the way the model is fitted to the data. Does this random effect add to explained variance compared to the fixed-effect-only model? m.noR = m.lin.noInt; anova(m.ri1, m.noR) ## Data: data.md.red ## Models: ## m.noR: Effect_Size ~ 0 + Condition ## m.ri1: Effect_Size ~ Condition + (1 | SubjectID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.noR 3 10229.9 10249 -5111.9 10223.9 ## m.ri1 4 7735.1 7761 -3863.6 7727.1 2496.7 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.4.2 Random intercept #2: experiment Another way to think about random effects is that they specify additional clusters in the data. For instance, most datapoints from participant A will be shifted above the mean, while most datapoints from participant B will be below the mean. Points A and points B will form two different clusters. The dataset we’re using has another obvious source of clusters - it was collected across multiple experiments. Although we would hope that the data we record would be unaffected by the specific experimental conditions, in practice this is rarely the case. The data might be collected on different MRI scanners, during different seasons, the Sentence &amp; Word blocks might have had different lengths, etc. If you have all this information and you think that each variable might affect your results, then you can go ahead and enter these terms directly. However, we here will just use Experiment. We assume that the mean responses to both conditions will vary across experiments (and, as before, across participants). To save space, from now on we will only print out fixed and random effect estimates. m.ri2 = lmer(Effect_Size ~ Condition + (1 | SubjectID) + (1 | Experiment), data = data.md.red, REML=FALSE); coef(summary(m.ri2)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.4563056 0.08056281 7.579646 5.663973 5.752963e-04 ## ConditionS -0.1119761 0.01526275 4603.939404 -7.336561 2.576588e-13 VarCorr(m.ri2) # random ## Groups Name Std.Dev. ## SubjectID (Intercept) 0.44329 ## Experiment (Intercept) 0.16633 ## Residual 0.52429 Previous model (m.ri1) reminder: ConditionW 0.47069 ConditionS -0.11198 We see again that changing the random effect structure affects the fixed effects too. Let’s compare our models to see if adding the experiment intercept actually improved model fit. anova(m.ri1, m.ri2) ## Data: data.md.red ## Models: ## m.ri1: Effect_Size ~ Condition + (1 | SubjectID) ## m.ri2: Effect_Size ~ Condition + (1 | SubjectID) + (1 | Experiment) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri1 4 7735.1 7761.0 -3863.6 7727.1 ## m.ri2 5 7709.9 7742.2 -3850.0 7699.9 27.236 1 1.801e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It did! We have a significant difference between the models, with model 2 having a higher log likelihood of the data. 3.4.3 Random intercept #3: region Finally, we know that different brain regions have different levels of BOLD activity: regions close to large vessels will have strong responses to everything. m.ri3 = lmer(Effect_Size ~ Condition + (1 | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE); coef(summary(m.ri3)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.4566623 0.09492157 12.84781 4.810944 3.514693e-04 ## ConditionS -0.1119761 0.01390210 4584.90113 -8.054616 1.007830e-15 VarCorr(m.ri3) # random ## Groups Name Std.Dev. ## SubjectID (Intercept) 0.44491 ## Region (Intercept) 0.21771 ## Experiment (Intercept) 0.16917 ## Residual 0.47755 Previous model (m.ri2) reminder: ConditionW 0.4563056 ConditionS -0.1119761 This time, the fixed effect estimates remained almost the same. But did adding a random intercept by region improve model fit further? Yes, it did. anova(m.ri2, m.ri3) ## Data: data.md.red ## Models: ## m.ri2: Effect_Size ~ Condition + (1 | SubjectID) + (1 | Experiment) ## m.ri3: Effect_Size ~ Condition + (1 | SubjectID) + (1 | Experiment) + ## m.ri3: (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri2 5 7709.9 7742.2 -3850.0 7699.9 ## m.ri3 6 6927.0 6965.8 -3457.5 6915.0 784.88 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.5 It gets better - random slopes Ok, our model is getting pretty complicated. But as you were reading about all those random intercepts, you might have been wondering - what if it’s not just the mean activity that varies across participants/regions/experiments? What if participant A not only has stronger activity overall but also has stronger responses to Sentences specifically? This kind of structure can be captured too. We call it a random slope because if we plot Condition on the x axis and Response on the y axis, the effect of interest will be the slope of the line going from the W value to the S value. High slope = steeper increase in activation as we go from W to S. Remember that so far we have been specifying our random effects as (1 | RandomVar). What does this mean? It means, take the random effect on the right and estimate how it varies according to the fixed effects on the left. So far we’ve just had 1 on the left, which is our intercept. But now we also want to say that our random variables can vary by condition, so we add it to the left side too. m.ris = lmer(Effect_Size ~ Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red, REML=FALSE); ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.0294832 ## (tol = 0.002, component 1) Uh oh, what happened here? We get a warning saying that our model failed to converge. That’s not good - it means that our effect estimates might be imprecise. What can we do to solve this? There are a few technical tricks we can use to force the model to converge (see section 4.3). But generally, the reason why our model here doesn’t converge is because it’s too complicated - we have too many variables and not enough data. So for now, let’s simplify and just add one random slope. m.ris = lmer(Effect_Size ~ Condition + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE); coef(summary(m.ris)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.4552242 0.09588469 13.57937 4.747622 0.0003385923 ## ConditionS -0.1088011 0.03287757 115.00448 -3.309282 0.0012494232 VarCorr(m.ris) # random ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.47456 ## ConditionS 0.32323 -0.343 ## Region (Intercept) 0.21798 ## Experiment (Intercept) 0.16834 ## Residual 0.44852 Note that the model also estimates the covariance between the random effect’s intercept and slope (in essence, an extra variable to take care of). Here, the intercept and the slope are correlated: participants with higher overall responses will also have a higher S&gt;W value. THINK: how can we modify equation (3.5) to incorporate the random slope by condition? Let’s check: did adding a random slope help beyond random intercepts? It did. anova(m.ris, m.ri3) ## Data: data.md.red ## Models: ## m.ri3: Effect_Size ~ Condition + (1 | SubjectID) + (1 | Experiment) + ## m.ri3: (1 | Region) ## m.ris: Effect_Size ~ Condition + (1 + Condition | SubjectID) + (1 | ## m.ris: Experiment) + (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri3 6 6927.0 6965.8 -3457.5 6915.0 ## m.ris 8 6567.3 6619.0 -3275.7 6551.3 363.69 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that here I decided to only keep the random slope by participant, and it happened to actually be helpful. If possible, you should be more systematic when deciding on model structure. For details, see section 4.3.4. 3.6 Fixed effects can be more complicated too I will leave you with a final twist on our model. So far we’ve only looked at Condition as our fixed effect: our goal was to get to random effects as soon as possible. But there is another important source of variation in our data - Hemisphere. Since there are only two hemispheres, there is no reason to include it as a fixed effect (in this case, we’d have to imagine left and right hemispheres as samples from an infinite population of hemispheres…). So this is going to be our second fixed effect. m.ris2 = lmer(Effect_Size ~ Condition + Hemisphere + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE); coef(summary(m.ris2)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.5154497 0.10594841 18.13017 4.865101 0.0001220173 ## ConditionS -0.1088011 0.03287916 114.99242 -3.309121 0.0012501139 ## HemisphereR -0.1204575 0.09446622 19.44581 -1.275139 0.2172850860 VarCorr(m.ris2) # random ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.47453 ## ConditionS 0.32325 -0.343 ## Region (Intercept) 0.20921 ## Experiment (Intercept) 0.16812 ## Residual 0.44852 Great, now we know that left and right hemisphere do not have significantly different overall activity levels1. But we also want to know whether our Condition effects vary by Hemisphere. What do we need to include? An interaction. m.ris3 = lmer(Effect_Size ~ Condition*Hemisphere + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE); coef(summary(m.ris3)) # fixed ## Estimate Std. Error df t value ## (Intercept) 0.49408827 0.10613083 18.29735 4.6554643 ## ConditionS -0.06607851 0.03537294 153.88236 -1.8680525 ## HemisphereR -0.07773581 0.09537081 20.18837 -0.8150901 ## ConditionS:HemisphereR -0.08544343 0.02608228 4469.92124 -3.2759190 ## Pr(&gt;|t|) ## (Intercept) 0.0001890031 ## ConditionS 0.0636561836 ## HemisphereR 0.4245326473 ## ConditionS:HemisphereR 0.0010612211 VarCorr(m.ris3) # random ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.47461 ## ConditionS 0.32334 -0.343 ## Region (Intercept) 0.20923 ## Experiment (Intercept) 0.16803 ## Residual 0.44798 Here, * is a shortcut meaning “estimate the main effects and the interaction”. Thus Condition*Hemisphere is equivalent to Condition + Hemisphere + Condition:Hemisphere, where : stands for the interaction itself. And hey, looks like the Sentence responses in the right hemisphere MD regions are weaker than in the left. Let’s do a final formal check to see whether adding the hemisphere terms was useful. anova(m.ris3, m.ris) ## Data: data.md.red ## Models: ## m.ris: Effect_Size ~ Condition + (1 + Condition | SubjectID) + (1 | ## m.ris: Experiment) + (1 | Region) ## m.ris3: Effect_Size ~ Condition * Hemisphere + (1 + Condition | SubjectID) + ## m.ris3: (1 | Experiment) + (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ris 8 6567.3 6619.0 -3275.7 6551.3 ## m.ris3 10 6559.1 6623.7 -3269.5 6539.1 12.28 2 0.002155 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yup, we explained additional variance. 3.7 Summary Fixed effects in a linear mixed effect model act just like regular regression terms. For categorical variables, the first level of a fixed effect variable acts like a baseline (unless you set the intercept to 0). Random effects are not simple regression terms; they are random variables. As such, you don’t estimate a value for each level (e.g. mean activation for each participant) but instead find the variance around the mean. For instance, you might estimate how much each participant’s data are likely to deviate from the mean value. You can estimate random intercepts (how much does a random effect influence the baseline) and random slopes (how much does a random effect modulate a fixed effect) You can decide whether adding a particular term improves model fit by conducting a likelihood ratio test. during word reading - see section 4.2.2↩ "],["the-devils-in-the-details.html", "4 The devil’s in the details 4.1 Testing significance 4.2 Contrasts 4.3 Convergence issues 4.4 Summary", " 4 The devil’s in the details Now you know the basics of how to build a mixed effect model. But in some ways, your learning has just begun. Now you will face a million choices that you will have to make for each model you make. We discuss some of them in this chapter. 4.1 Testing significance How do I know which terms in my model are significantly different from 0? We’ve already used some methods for estimating significance in the last chapter, but let’s be explicit here. 4.1.1 Method 1: Read the p-values directly from model summary Remember a term we had before, ConditionS? It is the estimate of the average Effect_Size for the Sentences &gt; Words contrast. How do we know whether this effect is significant? lmerTest allows us to just get a p value associated with each (thanks to this package, we get an extra column in your output for every fixed effect we estimate) m1 = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment) + (1 |Region), data = data.md.red, REML=FALSE) summary(m1) ## Linear mixed model fit by maximum likelihood . t-tests use ## Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) + ## (1 | Region) ## Data: data.md.red ## ## AIC BIC logLik deviance df.resid ## 6927.0 6965.8 -3457.5 6915.0 4714 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.2856 -0.5988 -0.0620 0.5180 5.9735 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SubjectID (Intercept) 0.19795 0.4449 ## Region (Intercept) 0.04740 0.2177 ## Experiment (Intercept) 0.02862 0.1692 ## Residual 0.22806 0.4776 ## Number of obs: 4720, groups: SubjectID, 115; Region, 20; Experiment, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.45666 0.09492 12.84781 4.811 0.000351 *** ## ConditionS -0.11198 0.01390 4584.90113 -8.055 1.01e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ConditionS -0.073 Look at Fixed effects -&gt; ConditionS -&gt; value in the Pr(&gt;|t|) column is the p value of the “S” contrast, which in our case is Sentence&gt;Word. Here’s how you can report this effect: The responses in the multiple demand (MD) network during sentence reading were lower than responses during word reading (beta=-0.11, SE=0.01, p&lt;.001). Note that lmerTest only estimates p values for fixed effects, so if you want to estimate the significance of a random effect, you have to use Method 2. 4.1.2 Method 2: Model comparison We’ve done this a lot in the previous chapter, but just to summarize: Create a “null” model where the predictor of interest is missing m1.null = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment), data = data.md.red, REML=FALSE) Use the likelihood ratio test (the anova() command) to find out if the model with more parameters fits the data better. anova(m1.null, m1) ## Data: data.md.red ## Models: ## m1.null: Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) ## m1: Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) + ## m1: (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m1.null 5 7709.9 7742.2 -3850.0 7699.9 ## m1 6 6927.0 6965.8 -3457.5 6915.0 784.88 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This tells you that Region matters but it doesn’t tell you anything about which Region has higher or lower responses compared to the rest. Here’s how you can describe this result: The overall response magnitude varied substantially by region (\\(\\sigma\\)=0.28). The likelihood ratio test showed that adding the region intercept term significantly improved model fit (\\(X^2\\)=784.9, p&lt;.001). 4.1.3 Method 3: pairwise tests If you just want the mean estimates for each condition and you want to compare them to 0, you can use ls_means() from lmerTest ls_means(m1) ## Least Squares Means table: ## ## Estimate Std. Error df t value lower upper Pr(&gt;|t|) ## ConditionW 0.456662 0.094922 12.8 4.8109 0.251350 0.661975 0.0003515 ## ConditionS 0.344686 0.094922 12.8 3.6313 0.139373 0.549999 0.0030993 ## ## ConditionW *** ## ConditionS ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Confidence level: 95% ## Degrees of freedom method: Satterthwaite Reporting: The MD network’s response to word reading was significantly above 0 (beta=0.41, SE=0.06, p&lt;.001), and so was the response to sentence reading (beta=0.27, SE = 0.09, p=.006). You can also use the same function to run a bunch of pairwise comparisons. In the current model, it was reasonably easy to read the effects right off the coefficients but when there are more than 2 levels per predictor this can get trickier so tools like this can be handy. ls_means(m1, pairwise = TRUE) ## Least Squares Means table: ## ## Estimate Std. Error df t value lower ## ConditionW - ConditionS 0.111976 0.013902 4584.9 8.0546 0.084721 ## upper Pr(&gt;|t|) ## ConditionW - ConditionS 0.139231 1.008e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Confidence level: 95% ## Degrees of freedom method: Satterthwaite THINK: how many rows would we get if we had three conditions (say, Word, Sentence and Paragraph)? Four? Five? Would we get all these estimates after running the lmer model? When models get more complicated you may need to switch to using emmeans, which is another package with lots of great ways to probe models. NOTE: when running follow-up analyses, don’t forget to account for multiple comparisons. Emmeans provides ways to do this automatically (by specifying the right parameters), but for lmerTest’s ls_means you’ll have to do it manually. If you have multiple correlated predictors, things get harder. The main thing that can happen is that the estimates become unstable. In that case, it’s safer to do model comparison (method 2). You can almost always find out if an effect explains useful variance or not, but you can’t always tell what the direction or magnitude of the effect is. 4.2 Contrasts 4.2.1 Intro to contrast coding Let’s take another look at the fixed effects in our model. m1 = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment) + (1 |Region), data = data.md.red, REML=FALSE) coef(summary(m1)) ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.4566623 0.09492157 12.84781 4.810944 3.514693e-04 ## ConditionS -0.1119761 0.01390210 4584.90113 -8.054616 1.007830e-15 As we’ve mentioned before, the first level (W) becomes the intercept, and all other levels are contrasted against it. But why is that? Well let’s see how R is treating these levels. Ultimately, in order to fit the model, R has to somehow convert them into numbers. Using the contrasts() function, you can see what those numbers are. contrasts(data.md.red$Condition) ## S ## W 0 ## S 1 This is called dummy coding - but there’s nothing dumb about it! It’s just the default in R. Another term for it is “treatment coding” But what if we don’t want to treat the word reading condition in this privileged way? Turns out, you can set the contrasts manually: data.md.red.sum = data.frame(data.md.red) # make a copy of the data contrasts(data.md.red.sum$Condition) = c(-0.5, 0.5) # change the contrast colnames(attr(data.md.red.sum$Condition, &quot;contrasts&quot;)) = &quot;S&gt;W&quot; # name the contrast (optional) contrasts(data.md.red.sum$Condition) ## S&gt;W ## W -0.5 ## S 0.5 Ok, looks confusing. What did we just do? The difference between S and W remains 1. What has changed is where we place 0 (aka, the intercept). In dummy coding, 0 was aligned with the W condition. Now, it is in between W and S. Thus, our intercept will now reflect the average response across both W and S conditions. This way of setting up contrasts is called sum coding (or deviation coding). m1.sum = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment) + (1 |Region), data = data.md.red.sum, REML=FALSE) coef(summary(m1.sum)) ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.4006743 0.09466671 12.71039 4.232473 1.027008e-03 ## ConditionS&gt;W -0.1119761 0.01390210 4584.90113 -8.054616 1.007830e-15 ConditionS&gt;W is still the difference between the 2 conditions. Does it really matter where we place the intercept? It matters whenever you want to report the intercept value. It matters even more when we want to interpret interactions. 4.2.2 Applications to interactions At the end of last chapter, we introduced a model with an interaction between 2 fixed effects - Condition and Hemisphere. Let’s take a closer look at what exactly it tells us about our data. We’ll start with a quick plot to see what results we should be expecting ggplot(data.md.red)+ stat_summary(aes(x=Hemisphere, y=Effect_Size, fill=Condition), geom=&quot;col&quot;, fun.y=&quot;mean&quot;, position=position_dodge(0.8))+ stat_summary(aes(x=Hemisphere, y=Effect_Size, group=Condition), geom=&#39;errorbar&#39;, fun.data=&#39;mean_se&#39;, position=position_dodge(0.8), width=0.2) Based on this plot, we would expect to see the main effect of condition (W&gt;S), maybe the main effect of hemisphere (L&gt;R), and likely an interaction between hemisphere and condition (the W&gt;S effect is larger in RH). Was that what we observed? m2 = lmer(Effect_Size ~ Condition*Hemisphere + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE); coef(summary(m2)) # fixed ## Estimate Std. Error df t value ## (Intercept) 0.49408827 0.10613083 18.29735 4.6554643 ## ConditionS -0.06607851 0.03537294 153.88236 -1.8680525 ## HemisphereR -0.07773581 0.09537081 20.18837 -0.8150901 ## ConditionS:HemisphereR -0.08544343 0.02608228 4469.92124 -3.2759190 ## Pr(&gt;|t|) ## (Intercept) 0.0001890031 ## ConditionS 0.0636561836 ## HemisphereR 0.4245326473 ## ConditionS:HemisphereR 0.0010612211 The interaction between condition and hemisphere is indeed significant (beta=-0.08, SE=0.03, p=.001), but the effect of condition is not (beta=-0.07, SE=0.04, p=.064), and neither is the effect of hemisphere (beta=-0.08, SE=0.10, p=.424). Why is that? Our intuition doesn’t align with the model output because the hemisphere and condition effects here do not actually reflect the main effect of each factor. The reason why is because each fixed effect is evaluated with respect to the intercept of the other one. Remember, this is the dataset where we’re using the default dummy coding, so the intercept for condition is words and the intercept for hemisphere is left. As a result, under ConditionW we have the simple effect of Condition: Words&gt;Sentences but only for the left hemisphere. And it’s indeed pretty small. under HemisphereR we have the simple effect of Hemisphere: Right&gt;Left but only for words. Looking at the plot, we can verify that the hemisphere difference for words is indeed ~0.08 (as predicted by our beta value). In order to estimate the main effects, we want to use sum coding. Then, the intercept will reflect the average response for each factor across levels. We’ve already done this for condition: contrasts(data.md.red.sum$Condition) ## S&gt;W ## W -0.5 ## S 0.5 Let’s do it for hemisphere too: contrasts(data.md.red.sum$Hemisphere) = c(-0.5, 0.5) colnames(attr(data.md.red.sum$Hemisphere, &quot;contrasts&quot;)) = &quot;R&gt;L&quot; contrasts(data.md.red.sum$Hemisphere) ## R&gt;L ## L -0.5 ## R 0.5 Now the effect of hemisphere will be estimated with respect to the average response for words and sentences, and the effect of condition will be estimated with respect to the average response across hemispheres. m2.sum = lmer(Effect_Size ~ Condition*Hemisphere + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red.sum, REML=FALSE); coef(summary(m2.sum)) ## Estimate Std. Error df t value ## (Intercept) 0.40082257 0.09337878 12.41336 4.292438 ## ConditionS&gt;W -0.10880025 0.03288109 114.96989 -3.308900 ## HemisphereR&gt;L -0.12045752 0.09446968 19.44370 -1.275092 ## ConditionS&gt;W:HemisphereR&gt;L -0.08544343 0.02608229 4469.91856 -3.275917 ## Pr(&gt;|t|) ## (Intercept) 0.0009697075 ## ConditionS&gt;W 0.0012510800 ## HemisphereR&gt;L 0.2173028893 ## ConditionS&gt;W:HemisphereR&gt;L 0.0010612282 Notice that the interaction remained the same but the simple effects changed. The effect of condition is now significant (beta=-0.11, SE=0.03, p=.001); the effect of hemisphere is not significant (beta=-0.12, SE=0.09, p=.217), but its estimate is larger and actually reflects the mean difference between L and R hemispheres. THINK: What does the intercept term reflect in m2 and in m2.sum? Of course, we don’t always want to transform everything to sum coding. It might be totally reasonable to estimate the effect of hemisphere for words rather than for the word/sentence average. You just want to be clear about how you’re coding your contrasts because it will have a big impact on how to interpret the results. 4.3 Convergence issues Remember we got a warning in section 3.5 saying that our model doesn’t converge? We’ll repeat that example here: m.ris = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red, REML=FALSE); ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.0352573 ## (tol = 0.002, component 1) coef(summary(m.ris)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.4547675 0.08210915 9.847007 5.538573 0.0002621747 ## ConditionS 0.3378952 0.10445966 13.926610 3.234696 0.0060276813 VarCorr(m.ris) # random ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.479932 ## ConditionS 0.318837 -0.364 ## Region (Intercept) 0.184415 ## ConditionS 0.080952 0.795 ## Experiment (Intercept) 0.132303 ## ConditionS 0.059135 0.818 ## Residual 0.446580 Let’s take a look at the strategies we can use to resolve this issue. 4.3.1 Solution 1: try other optimizers We can use a different optimizer and/or explicitly specify the stopping criterion. m.ris.bobyqa = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red, REML=FALSE, control = lmerControl(optimizer = &quot;bobyqa&quot;, optCtrl=list(maxfun=2e5))) coef(summary(m.ris.bobyqa)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.4544214 0.08190374 8.989032 5.548237 0.0003587971 ## ConditionS 0.3378332 0.10455535 13.765356 3.231143 0.0061498913 VarCorr(m.ris.bobyqa) # random ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.480476 ## ConditionS 0.319156 -0.366 ## Region (Intercept) 0.184321 ## ConditionS 0.080824 0.795 ## Experiment (Intercept) 0.131508 ## ConditionS 0.058361 0.863 ## Residual 0.446573 It worked! No more convergence issues. What if bobyqa didn’t converge? Instead of just specifying a single optimizer, you can use the function allFit which will run all the optimizers for you, and hopefully at least some of them fit. See lme4 documentation for more details. allFit can also be used to determine whether you can trust your estimates. For this, you can fit the model with multiple optimizers, look at the fixed effects and determine whether the estimates across all optimizers are the same (out to ~4 sig digits). If they are, then it’s probably fine to use the model estimates. 4.3.2 Solution 2: brms https://paul-buerkner.github.io/brms/ This is a package for Bayesian modeling; it is based on the probabilistic programming language called Stan. The syntax for specifying the model is very similar to lme4, but switching from a frequentist to a Bayesian approach to modeling might require some extra work. These models might also take a while to run. 4.3.3 Solution 3: REML In all the examples above, we fit our model using maximum likelihood. The default for lmer is actually restricted maximum likelihood (REML). Because it fits the data in stages, in some cases it might make it easier for the model to converge. m.ris.reml = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red, REML=TRUE) summary(m.ris.reml) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + ## Condition | Experiment) + (1 + Condition | Region) ## Data: data.md.red ## ## REML criterion at convergence: 6527 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.6493 -0.5807 -0.0683 0.5089 5.9141 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## SubjectID (Intercept) 0.229264 0.47882 ## ConditionS 0.101223 0.31816 -0.36 ## Region (Intercept) 0.034497 0.18573 ## ConditionS 0.006672 0.08168 0.79 ## Experiment (Intercept) 0.023741 0.15408 ## ConditionS 0.005286 0.07270 0.47 ## Residual 0.199429 0.44657 ## Number of obs: 4720, groups: SubjectID, 115; Region, 20; Experiment, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.45783 0.08837 8.54662 5.181 0.000683 *** ## ConditionS 0.33841 0.10926 11.06806 3.097 0.010086 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW ## ConditionS 0.904 This method worked too. Note, however, that if you then try to use the anova() function on the model, it will automatically refit using maximum likelihood - which brings us back to square one. 4.3.4 Solution 4: simplify your model The convergence issues can arise if your model is too complicated and/or if there isn’t enough data to estimate all the parameters. THINK: how many parameters does m.ris need to estimate? (Hint: don’t forget about the correlations between random effects) There is no one answer to how many samples are sufficient, but if it’s 2 per parameter, it’s too low (e.g. we’re estimating the participant by condition slope, and we only have estimates for 2 ROIs, it’s hard to decide how much variance should go into ROI terms and how much should be assigned to the participant x condition slope). Note that it’s not about the number of datapoints but rather the number of groupings. In some cases, you will get a singular fit warning. We can also check for it explicitly: isSingular(m.ris) ## [1] FALSE A singular model means that one of the random effects (or a combination of random effects) is zero. While mathematically such a model is well-defined, in practice this model often means that the model overfits: there is not enough power to distinguish All in all, if you suspect that your model is at risk of overfitting the data, you should consider simplifying your model. A general recommendation is to first remove random effects which account for least variance. VarCorr(m.ris) ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.479932 ## ConditionS 0.318837 -0.364 ## Region (Intercept) 0.184415 ## ConditionS 0.080952 0.795 ## Experiment (Intercept) 0.132303 ## ConditionS 0.059135 0.818 ## Residual 0.446580 (Admittedly, this step is a bit weird since the reason we’re simplifying the model is because we don’t trust these estimates to begin with. You can also try estimating these random effects individually in simpler models &amp; then comparing the magnitudes.) Our lowest random effect is the Condition x Experiment slope, so let’s try removing that: m.ris.red = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 | Experiment) + (1 + Condition | Region), data = data.md.red, REML=FALSE) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.00201745 ## (tol = 0.002, component 1) Okay that didn’t do it… Let’s try one more - the variance estimate for the Condition x Region slope is also pretty low. m.ris.red = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red, REML=FALSE) coef(summary(m.ris.red)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.4552242 0.09588469 13.57936 4.747622 0.0003385931 ## ConditionS 0.3464231 0.09585435 13.57702 3.614057 0.0029489722 VarCorr(m.ris.red) # random ## Groups Name Std.Dev. Corr ## SubjectID (Intercept) 0.47456 ## ConditionS 0.32323 -0.343 ## Region (Intercept) 0.21798 ## Experiment (Intercept) 0.16834 ## Residual 0.44852 Alright, now our model finally converges. In short, here is what you should do when encountering convergence issues: Make sure your model structure is justified (more on that in the next chapter). Try to make the model converge by tinkering with the optimization procedure. 4.4 Summary You can estimate the significance of fixed effects using the lmerTest package; besides the p value, we also recommend reporting the effect size and the standard error of the estimate. You can estimate the significance of a random effect by performing the likelihood ratio test; note, however, that this test does not allow you to estimate the direction or the magnitude of the effect. Be mindful of the contrasts you’re setting for your categorical variables (explicitly or implicitly). When running into convergence issues, make sure your model structure makes sense given your dataset and then try one of the technical solutions, e.g. a different optimizer. "],["meta-decisions.html", "5 Meta Decisions 5.1 Brain regions: fixed or random? 5.2 Model complexity 5.3 Single-subject mixed analyses? 5.4 Conclusion", " 5 Meta Decisions Now that we’ve covered the foundations of how to build mixed effect models and how to interpret their results, we would like to cover some of the more conceptual questions. We are entering the uncertain terrain where, for many questions, instead of yes or no, we’ll have to say “it depends”. But this is what modeling and statistics is all about - making certain assumptions about how your data is structured and then building your model based on these assumptions. 5.1 Brain regions: fixed or random? So far, we have made it seem obvious whether a variable should be entered as a fixed effect or a random effect. For Condition, we had 2 levels and wanted to examine each one - hence our decision to include it as a fixed effect. For Participant, we had many levels and only wanted to estimate general variance - thus it became a random variable. But what if the case is more borderline? For instance, what about brain regions? Do we care about each brain region individually or do we want to estimate general variability across regions? In our models so far, we estimated the effect of condition on activity in the MD network. Since we treated the network as its own entity, we were content with evaluating the overall effect of condition; region was entered as a random effect to “absorb” additional variance. Plus, we had 20 different regions, so we weren’t necessarily keen on having 19 region effects to report (we chose to estimate the effect of hemisphere instead). Let’s take a look at the other half of the dataset - the language network responses (left hemisphere only). data = read.csv(&#39;data/Diachek2020.csv&#39;, header=TRUE) data.lang = subset(data, (System==&quot;language&quot;)) data.lang = subset(data.lang, (Hemisphere==&quot;L&quot;)) data.lang$Condition = factor(data.lang$Condition, levels=c(&#39;W&#39;, &#39;S&#39;)) # plot ggplot(data.lang)+ stat_summary(aes(x=Condition, y=Effect_Size), fun.y=&quot;mean&quot;, geom=&quot;col&quot;)+ facet_wrap(~Region, ncol = 5)+ labs(title=&quot;By Region&quot;)+ theme(legend.position = &quot;none&quot;, plot.title=element_text(face=&quot;bold&quot;)) Should we model each region as a fixed effect or a random effect? Five regions is pretty low for a random effect but we could probably pull it off. The real question is: do we want to treat each region as an independent entity, which will be estimated independently from the rest, or do we want to treat them as part of the overall brain region population, in which case the model will estimate a joint variance parameter for all of them? The answer is: it depends on your assumptions and goals as a researchers. Are you interested in reporting the overall effect across all these regions? Do you think that they come from the same population? Or do you think that your regions of interest in anterior and posterior temporal lobes behave independently from each other? Below, we will try to model brain region both ways and see what can come out of it. 5.1.1 As a fixed effect Since we do not have an obvious region that would serve as the baseline, we will set the intercept to 0. m.fixedreg = lmer(Effect_Size ~ 0 + Region + Condition + (1 |SubjectID) + (1 |Experiment), data = data.lang, REML=FALSE) coef(summary(m.fixedreg)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## RegionLAntTemp 0.3110634 0.07208085 51.73055 4.315478 7.199986e-05 ## RegionLIFG 0.7948357 0.07208085 51.73055 11.027002 3.422351e-15 ## RegionLIFGorb 0.5588573 0.07208085 51.73055 7.753200 3.234654e-10 ## RegionLMFG 0.9460898 0.07208085 51.73055 13.125396 4.305164e-18 ## RegionLPostTemp 0.6154249 0.07208085 51.73055 8.537981 1.891258e-11 ## ConditionS 0.4349757 0.03566706 3611.53937 12.195445 1.488782e-33 VarCorr(m.fixedreg) # random ## Groups Name Std.Dev. ## SubjectID (Intercept) 0.51782 ## Experiment (Intercept) 0.32869 ## Residual 0.62866 We get the estimate for each of the five regions; these estimates are evaluated at the baseline level of condition, which is words. The last fixed effect, ConditionS, here estimates the magnitude of the Sentences&gt;Words contrast at the baseline level for region (which happens to be LAntTemp: we didn’t specify it, so R just picked the first region alphabetically)s. So we have 5 regions’ responses to words and one region’s response to sentences&gt;words. Not very useful. You can improve the interpretation of these effects by changing the contrasts (see section 4.2). You would also probably want to include the condition x region interaction term to estimate the sentences&gt;words response in all regions. For now, let’s just estimate the main effects with ls_means: ls_means(m.fixedreg) ## Least Squares Means table: ## ## Estimate Std. Error df t value lower upper ## RegionLAntTemp 0.528551 0.067984 41.5 7.7746 0.391305 0.665798 ## RegionLIFG 1.012324 0.067984 41.5 14.8906 0.875077 1.149570 ## RegionLIFGorb 0.776345 0.067984 41.5 11.4195 0.639098 0.913592 ## RegionLMFG 1.163578 0.067984 41.5 17.1154 1.026331 1.300824 ## RegionLPostTemp 0.832913 0.067984 41.5 12.2516 0.695666 0.970159 ## ConditionW 0.645254 0.069975 46.0 9.2212 0.504397 0.786111 ## ConditionS 1.080230 0.066219 37.3 16.3129 0.946090 1.214370 ## Pr(&gt;|t|) ## RegionLAntTemp 1.254e-09 *** ## RegionLIFG &lt; 2.2e-16 *** ## RegionLIFGorb 2.186e-14 *** ## RegionLMFG &lt; 2.2e-16 *** ## RegionLPostTemp 2.267e-15 *** ## ConditionW 5.077e-12 *** ## ConditionS &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Confidence level: 95% ## Degrees of freedom method: Satterthwaite Ok, this is better. We get the main effect of each region (averaged across conditions) and the main effect of each condiion (averaged across regions). Had we included an interaction, we would also have estimates for region-specific differences in word and sentence responses. This seems sufficient to describe our results. The important thing is that we get a separate estimate for each ROI. If the average responses in 4 ROIs were 0.1-0.2 and the response in LIFG was 2.5, the model would just fit the corresponding coefficients. There is no assumption that activity levels across different ROIs need to be comparable. On the other hand, our ability to estimate whole-network effects is somewhat limited. For instance, we don’t have the sentences&gt;words estimate for the entire network. We can specify it in follow-up tests but it might be a bit cumbersome. The value of this approach is the focus on individual regions, not so much the network as a whole. 5.1.2 As a random effect Ok, but what if we mostly care about the network-level responses? Then we can treat ROI as a random effect. If we decide to evaluate the responses in individual ROIs, we can always run follow-up tests. (see how I switched the optimizer here to make the model converge?) m.randomreg = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment) + (1 |Region), data = data.lang, REML=FALSE, control = lmerControl(optimizer = &quot;bobyqa&quot;, optCtrl=list(maxfun=2e5))) coef(summary(m.randomreg)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.6452897 0.12174795 10.61769 5.30021 2.841796e-04 ## ConditionS 0.4349022 0.03570038 3624.26797 12.18200 1.735365e-33 VarCorr(m.randomreg) # random ## Groups Name Std.Dev. ## SubjectID (Intercept) 0.51781 ## Experiment (Intercept) 0.33253 ## Region (Intercept) 0.22181 ## Residual 0.62893 This should look familiar - we’ve run versions of this model a lot using the MD data. The intercept is the effect of word reading across all regions, ConditionS is the sentence&gt;words response across all regions, and Regions (Intercept) is the standard deviation of each region’s mean response from the grand mean. This model is designed for making statements like “the language network responds to X”. If we want to see whether there is significant variation in the mean response across ROIs, we can run the likelihood ratio test: m.randomreg.null = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment), data = data.lang, REML=FALSE, control = lmerControl(optimizer = &quot;bobyqa&quot;, optCtrl=list(maxfun=2e5))) anova(m.randomreg, m.randomreg.null) ## Data: data.lang ## Models: ## m.randomreg.null: Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) ## m.randomreg: Effect_Size ~ 1 + Condition + (1 | SubjectID) + (1 | Experiment) + ## m.randomreg: (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.randomreg.null 5 11951 11984 -5970.5 11941 ## m.randomreg 6 11394 11434 -5691.2 11382 558.53 1 &lt; 2.2e-16 ## ## m.randomreg.null ## m.randomreg *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Reporting: &gt; The random intercept by ROI explained a significant amount of variance (\\(\\sigma\\)=0.22, likelihood ratio test \\(X^2\\)=558.5, p&lt;.001). To estimate the sentences&gt;words effect in individual regions, you can run a follow-up test on the data from a single region: data.lang.IFG = subset(data.lang, (Region==&quot;LIFG&quot;)) m.randomreg.IFG = lmer(Effect_Size ~ 1 + Condition + (1 |SubjectID) + (1 |Experiment), data = data.lang.IFG, REML=FALSE) coef(summary(m.randomreg)) # fixed ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.6452897 0.12174795 10.61769 5.30021 2.841796e-04 ## ConditionS 0.4349022 0.03570038 3624.26797 12.18200 1.735365e-33 VarCorr(m.randomreg) # random ## Groups Name Std.Dev. ## SubjectID (Intercept) 0.51781 ## Experiment (Intercept) 0.33253 ## Region (Intercept) 0.22181 ## Residual 0.62893 Now we have the words effect and the sentences&gt;words effect specifically for IFG. (When performing the follow-up tests for all regions, don’t forget to correct for multiple comparisons). 5.1.3 Beyond brain region In general, for every predictor you enter into the model, you should consider whether it makes more sense to make it a fixed or a random effect. How many levels does it have? Are those levels independent or do they come from the same distribution? Are we interested in estimating each level separately or in getting an overall variance estimate? Sometimes the answers will all point in the same direction and sometimes they won’t. The important thing is to make a principled decision and to be able to justify it. 5.2 Model complexity A big question when deciding on the model structure is: how complex should it be? Should we only include the terms we care about? The terms that we know contribute to the variance? Any terms that might potentially affect the result? Statisticians differ in their opinions on this topic. A famous paper by Barr et al. (2013) argues that we should “keep it maximal”: models that are overly conservative in their random effects structure generalize more poorly. Their recommendation is therefore to include the maximum number of terms justified by the design. In practice, of course, you may quickly run into convergence issues or risk overfitting your model. A reasonable middle ground is to start with a maximal model and then iteratively simplify it (see section 4.3.4). However, this means that we have to sacrifice potentially important model terms due to the practical limitations of our dataset. Ideally, we would estimate the ideal model structure in advance and design the experiment with it in mind. The fMRI community has only recently begun applying mixed effect models in our work. Hopefully, in a few years the community will have established the optimal model structure for our specific use cases and validated it across a range of experiments. 5.3 Single-subject mixed analyses? All the analyses we describe here are conducted at the group level. The Effect_Size variable consists of beta values for individual subjects obtained with a classical general linear model (for a video refresher on the GLM, check out this video tutorial). However, given how powerful mixed effect models are, don’t we want to incorporate them into subject-level analyses too? For instance, we could model variation across items and/or across experimental runs. The answer is: yes, in theory this would be great. Our group doesn’t do that yet, but we might eventually - stay tuned! Although the potential utility of mixed effect models at the single subject levels depends quite a lot on the specifics of your experimental design. If we do decide to introduce mixed effect modeling at the single subject level, the next question becomes: do we want to keep the two-stage processing pipeline (subject-level and then group-level) or do we want to throw in all the effects in one big model? Koh et al (2018) describe a relatively straightforward way of joint subject- and group-level timeseries modeling with mixed effect models. The only issue is that these models become computationally expensive really quickly and might have trouble scaling. The two-stage model is computationally much more feasible (which is why it is almost universally used in fMRI analyses today). It is also more flexible - single subject estimates can potentially be used for many different group models. However, Jeanette Mumford in her 2020 video points out a few downsides of breaking down this process into two stages, such as inaccuracies due to ignoring within-subject variance. Thus, this debate is also far from settled. 5.4 Conclusion All in all, this is an exciting time. The mixed effects modeling approach has conquered many fields and is now making steady advances in neuroimaging. Many practices remain to be developed, ratified and accepted by the community, but we can already use mixed effect models to account for previously ignored structure in our data. Hopefully this tutorial provided a helpful introduction to the topic! "]]
