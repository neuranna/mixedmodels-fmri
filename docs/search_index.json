[["index.html", "index 1 INTRODUCTION", " index Anna Ivanova 2020-11-25 1 INTRODUCTION Here I will tell you what this manual is all abou "],["building-your-first-mixed-model.html", "2 Building your first mixed model 2.1 Load data 2.2 Building from the ground up 2.3 A model with 1 fixed effect 2.4 Was it worth it? Model comparison 2.5 Adding the first random effect: participant", " 2 Building your first mixed model # Loading R packages # BEGINNNER TIP: before loading the package for the first time, you need to install it # using the command install.packages(&quot;myPackageNameInQuotes&quot;) library(bookdown) library(lme4) library(multcomp) library(tidyverse) library(lmerTest) library(readxl) library(optimx) # Checking the version of the &quot;lme4&quot; package packageVersion(&quot;lme4&quot;) ## [1] &#39;1.1.21&#39; 2.1 Load data # load data data = read.csv(&#39;data/Diachek2020.csv&#39;, header=TRUE); print(str(data)) ## &#39;data.frame&#39;: 31529 obs. of 9 variables: ## $ Experiment : Factor w/ 30 levels &quot;Experiment1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ System : Factor w/ 2 levels &quot;language&quot;,&quot;MD&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Region : Factor w/ 30 levels &quot;LAntTemp&quot;,&quot;LH_antParietal&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ SubjectID : Factor w/ 679 levels &quot;007_KAN_parametric_07&quot;,..: 2 10 11 12 56 74 96 107 111 169 ... ## $ Condition : Factor w/ 2 levels &quot;S&quot;,&quot;W&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Hemisphere : Factor w/ 2 levels &quot;L&quot;,&quot;R&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Effect_Size: num 0.602 -0.303 -0.369 0.361 -0.194 ... ## $ Modality : Factor w/ 2 levels &quot;auditory&quot;,&quot;visual&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Task : Factor w/ 2 levels &quot;passive&quot;,&quot;task&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## NULL # Explicitly specifying all categorical variables as factors. # Generally, R will default to treating numbers as type &quot;numeric&quot; and strings as factor, but it helps to be explicit # COMMON MISTAKE: if participant IDs are numbers, they will be treating as numeric # (i.e. participant 2 &gt; participant 1) data$Experiment = factor(data$Experiment); data$System = factor(data$System); data$Region = factor(data$Region); data$SubjectID = factor(data$SubjectID); data$Hemisphere = factor(data$Hemisphere); data$Modality = factor(data$Modality); data$Task = factor(data$Task); You can also use the ‘factor’ command to specify the order of the different values (“levels”) thata given variable can take. If you don’t specify the order, the levels will be ordered alphabetically. Why does it matter? The first level will be treated as the reference level (baseline) by the lme model. See below and the “contrasts” section for more details. data$Condition = factor(data$Condition, levels=c(&#39;W&#39;, &#39;S&#39;)); Let’s inspect our data! summary(data) ## Experiment System Region ## Experiment1 :11610 language:10569 LAntTemp : 1057 ## Experiment15: 2370 MD :20960 LIFG : 1057 ## Experiment23: 1979 LIFGorb : 1057 ## Experiment24: 1260 LMFG : 1057 ## Experiment19: 1020 LPostTemp: 1057 ## Experiment20: 960 RAntTemp : 1057 ## (Other) :12330 (Other) :25187 ## SubjectID Condition Hemisphere Effect_Size ## 117_KAN_EvDB_20150217c: 150 W: 6899 L:15765 Min. :-7.48391 ## 210_KAN_EvDB_20150223d: 150 S:24630 R:15764 1st Qu.:-0.09324 ## 211_KAN_EvDB_20150217b: 150 Median : 0.29143 ## 214_KAN_evDB_20141103b: 150 Mean : 0.40459 ## 253_KAN_EvDB_20150211a: 150 3rd Qu.: 0.80658 ## 254_KAN_EvDB_20150211c: 150 Max. : 6.85222 ## (Other) :30629 ## Modality Task ## auditory: 3630 passive:20550 ## visual :27899 task :10979 ## ## ## ## ## To simplify things a bit, we will create a reduced dataset of MD only, and another with only experiments 19-24 data.md = subset(data, (System==&quot;MD&quot;)); for (ii in 19:24) { x = subset(data.md, Experiment == paste(&quot;Experiment&quot;, as.character(ii), sep=&quot;&quot;)); if (ii==19){ data.md.red = x; } else { data.md.red = rbind(data.md.red,x); } } summary(data.md.red) ## Experiment System Region ## Experiment23:1320 language: 0 LH_antParietal : 236 ## Experiment24: 840 MD :4720 LH_insula : 236 ## Experiment19: 680 LH_medialFrontal: 236 ## Experiment20: 640 LH_midFrontal : 236 ## Experiment21: 640 LH_midFrontalOrb: 236 ## Experiment22: 600 LH_midParietal : 236 ## (Other) : 0 (Other) :3304 ## SubjectID Condition Hemisphere Effect_Size ## 365_FED_20170510a_3T2: 80 W:2360 L:2360 Min. :-2.845794 ## 498_FED_20170510b_3T2: 80 S:2360 R:2360 1st Qu.:-0.000482 ## 541_FED_20170523d_3T2: 80 Median : 0.371629 ## 007_KAN_parametric_07: 40 Mean : 0.418242 ## 018_FED_20151202b_3T1: 40 3rd Qu.: 0.790246 ## 018_FED_20151203a_3T1: 40 Max. : 4.048679 ## (Other) :4360 ## Modality Task ## auditory: 0 passive:1480 ## visual :4720 task :3240 ## ## ## ## ## 2.2 Building from the ground up Let’s start with the simplest linear model: only an intercept. This looks like: \\[\\begin{equation} \\vec{y} = \\begin{bmatrix} 1\\\\...\\\\1 \\end{bmatrix} * \\beta_0 + \\epsilon \\tag{2.1} \\end{equation}\\] wherу \\(\\vec{y}\\) is a vector with the measurements we want to model (in our example, Effect_Size), \\(\\beta_0\\) is the baseline response, and \\(\\epsilon\\) is residual error. The length of the ones vector is the same as the number of observations in \\(\\vec{y}\\). Essentially, this model is predicting the same number for every single measurement. In R syntax, we can say that \\(\\vec{y}\\) is proportional to some value (and estimate the coefficients that make this proportion work out). In this case, we assume that each value in \\(\\vec{y}\\) is a constant and therefore proportional to 1: \\[\\begin{equation} y \\propto 1 \\tag{2.2} \\end{equation}\\] Here’s what it looks like in the code: # specify the model m.lin.noCond = lm(Effect_Size ~ 1, data = data.md.red) # show the result summary(m.lin.noCond) ## ## Call: ## lm(formula = Effect_Size ~ 1, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2640 -0.4187 -0.0466 0.3720 3.6304 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.41824 0.01044 40.08 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.717 on 4719 degrees of freedom 2.3 A model with 1 fixed effect Let’s add a single fixed predictor. Now our effect size \\(\\vec{y}\\) depends not only on the baseline response, but also on the experimental condition. The general formula for such a design looks like this: \\[\\begin{equation} y = X * \\vec{\\beta} + \\epsilon \\tag{2.3} \\end{equation}\\] What’s X? X is the design matrix; it specifies our fixed effects for this model (fixed effects are like regular regression terms - in fact, if you’ve seen matrix-form regression equations before, this should all be familiar). The size of X will depend on the number of conditions. How many conditions do we have here? str(unique(data$Condition)) ## Factor w/ 2 levels &quot;W&quot;,&quot;S&quot;: 2 1 We have 2 conditions, words (W) and sentences (S). So our design matrix will look something like the following: \\[\\begin{equation} X = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ ...\\\\ 1 &amp; 1 \\end{bmatrix} \\tag{2.4} \\end{equation}\\] where the first column is our intercept (“baseline” neural activity) and the second column has 1 whenever that row comes from the sentence condition and 0 otherwise. Where is the word condition? Remember that we said earlier that the first level of the factor variable gets treated as the baseline. So here, the response to words is our baseline, and the response to sentences is the stuff you would get “on top” of the word-induced activation - in other words, the magnitude of the S&gt;N contrast. The intercept, in turn, would give you the average magnitude of the W&gt;baseline contrast. This might sound a bit counterintuitive, but imagine if we tried to specify the intercept, word and sentence as 3 separate effects to estimate. Then we could still compare sentences to words, but would have trouble distinguishing the words from the intercept. If our observed response to words is 1.2, does the intercept contribute 0 and words 1.2? intercept 1.2 and words 0? intercept 0.6 and words 0.6? There is an infinite number of combinations. (Mathematically, we can state that the matrix (2.4) would be rank-deficient if we added the predictor variable for S, since it’s a direct complement of W. Thus, there would be an infinite number of coefficients \\(\\beta\\) for solving equation (2.3)) Bottom line: if you have n levels, you can estimate the maximum of n effects. Ok, with all those preliminaries behind us, let’s finally run the model! m.lin = lm(Effect_Size ~ Condition, data = data.md.red) summary(m.lin) ## ## Call: ## lm(formula = Effect_Size ~ Condition, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2080 -0.4233 -0.0426 0.3806 3.5744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.47423 0.01471 32.228 &lt; 2e-16 *** ## ConditionS -0.11198 0.02081 -5.381 7.77e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7148 on 4718 degrees of freedom ## Multiple R-squared: 0.0061, Adjusted R-squared: 0.005889 ## F-statistic: 28.95 on 1 and 4718 DF, p-value: 7.77e-08 We do not need to explicitly specify an intercept: a model with +1 is the same as a model without it m.lin.int = lm(Effect_Size ~ 1 + Condition, data = data.md.red); summary(m.lin.int) ## ## Call: ## lm(formula = Effect_Size ~ 1 + Condition, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2080 -0.4233 -0.0426 0.3806 3.5744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.47423 0.01471 32.228 &lt; 2e-16 *** ## ConditionS -0.11198 0.02081 -5.381 7.77e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7148 on 4718 degrees of freedom ## Multiple R-squared: 0.0061, Adjusted R-squared: 0.005889 ## F-statistic: 28.95 on 1 and 4718 DF, p-value: 7.77e-08 We can explicitly supress the intercept. The resulting model is the same in terms of its predicted y values, but the coefficients and their interpretation are different. THINK: Which two contrasts does this no-intercept model evaluate? (Hint: you can compare these numbers with the numbers produced by the previous model) m.lin.noInt = lm(Effect_Size ~ 0 + Condition, data = data.md.red); summary(m.lin.noInt) ## ## Call: ## lm(formula = Effect_Size ~ 0 + Condition, data = data.md.red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2080 -0.4233 -0.0426 0.3806 3.5744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## ConditionW 0.47423 0.01471 32.23 &lt;2e-16 *** ## ConditionS 0.36225 0.01471 24.62 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7148 on 4718 degrees of freedom ## Multiple R-squared: 0.2585, Adjusted R-squared: 0.2582 ## F-statistic: 822.4 on 2 and 4718 DF, p-value: &lt; 2.2e-16 2.4 Was it worth it? Model comparison Does the model with Condition as a fixed effect explain more variance than the intercept-only model? To find out, we can compare the two models using the anova function. This function implements the likelihood ratio test. NOTE: the likelihood ratio (LR) test only works for nested models, i.e. models that have the same structure except that one model has some additional terms. The LR test would then tell us whether these terms are worth adding - in formal words, whether they significantly improve model fit. If you want to compare non-nested models, you can use other methods, such as AIC. anova(m.lin.noCond, m.lin) ## Analysis of Variance Table ## ## Model 1: Effect_Size ~ 1 ## Model 2: Effect_Size ~ Condition ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 4719 2425.7 ## 2 4718 2410.9 1 14.796 28.954 7.77e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5 Adding the first random effect: participant Now we’re ready to actually build a mixed effect model. Our previous model assumed that all our participants had the same response to the S and W conditions - any variation was put in the error term. However, we know that some participants tend to have generally high responses and vice versa; thus, we might expect someone with a really high response to words to also have a high response to sentences. Can we incorporate that knowledge into the model? We can! Let’s tell the model that the intercept varies across participants by adding an additional term to it: \\[\\begin{equation} y = X * \\vec{\\beta} + \\begin{bmatrix} 1\\\\...\\\\1 \\end{bmatrix} * \\vec{b} + \\epsilon \\tag{2.5} \\end{equation}\\] Here, \\(\\vec{b}\\) specifies a participant-specific offset in the response strength - some will be above average, and some below average. This offset is not condition-specific (yet). Why do we call it a random effect? Unlike Condition, each \\(\\vec{b}\\) value is participant-specific. Still, we could just add a bunch of columns to X for each participant and put 1’s against all trials completed by any particular participant. Then our participant-specific estimates would just be added to \\(\\vec{\\beta}\\). In some cases, modeling participant-specific variation as a fixed effect is a valid approach. However, in our case, we have many participants but not much participant-specific data, so we would just get a lot of noisy effects. Moreover, all the-participant specific terms have something in common - they are not fully independent. For example, if your participant intercepts are 1, 1.1, 0.9, 1.2, you have a pretty strong hunch that the next one is not going to be 15. Mathematically, we can state that participant-specific intercepts come from a normal distribution where the mean is the group intercept (estimated as the first term in \\(\\vec{\\beta}\\)), and the variance is a free parameter we need to estimate. Thus, our \\(\\vec{b}\\) term is never evaluated directly! It is a random variable of the form \\[\\begin{equation} \\vec{b} \\sim \\mathcal{N}(0,\\sigma^{2}) \\end{equation}\\] (the mean is 0 because this term is only estimating the participant-specific offset from the overall intercept) With this ‘random effect’ trick, we achieve two goals: - we estimate the intercept effects across all participants by specifying just one parameter - ^{2} - we leverage the power of all the dataset, not just the participant-specific data. Thus if any one participant’s values are unusually high, it would not have a strong result on the model - the normal distribution will “regularize” the estimates for that participant. LET’S RUN OUR FIRST MIXED EFFECT MODEL! The previous models did not have any random terms, and so we evaluated it using the ‘lm’ function. For mixed models, we use ‘lmer’. # I want to evaluate the model by maximum likelihood, not restricted maximum likelihood (REML), # so I&#39;m setting REML to FALSE m.ri1 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID), data = data.md.red, REML=FALSE); summary(m.ri1) ## Linear mixed model fit by maximum likelihood . t-tests use ## Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: Effect_Size ~ 0 + Condition + (1 | SubjectID) ## Data: data.md.red ## ## AIC BIC logLik deviance df.resid ## 7735.1 7761.0 -3863.6 7727.1 4716 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.4796 -0.5633 -0.0710 0.4803 5.7666 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SubjectID (Intercept) 0.2365 0.4864 ## Residual 0.2758 0.5252 ## Number of obs: 4720, groups: SubjectID, 115 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.47069 0.04663 121.41170 10.094 &lt; 2e-16 *** ## ConditionS 0.35872 0.04663 121.41170 7.693 4.24e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW ## ConditionS 0.946 Note that the fixed effects structure is the same as before, but now we also have the “random effects” section, which tells us how much variance was explained by the intercept varying across participants and how much is left in the residuals. The fixed effect estimates themselves have changed - adding the random effect changes the way the model is fitted to the data. Does this random effect add to explained variance compared to the fixed-effect-only model? m.noR = m.lin.noInt; anova(m.ri1, m.noR) ## Data: data.md.red ## Models: ## m.noR: Effect_Size ~ 0 + Condition ## m.ri1: Effect_Size ~ 0 + Condition + (1 | SubjectID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.noR 3 10229.9 10249 -5111.9 10223.9 ## m.ri1 4 7735.1 7761 -3863.6 7727.1 2496.7 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.1 Now add random intercepts by experiment Baseline model reminder: ConditionS 0.36225 ConditionW 0.47423 m.ri2 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment), data = data.md.red); summary(m.ri2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) ## Data: data.md.red ## ## REML criterion at convergence: 7709.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.4973 -0.5635 -0.0702 0.4803 5.7706 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SubjectID (Intercept) 0.19666 0.4435 ## Experiment (Intercept) 0.03391 0.1841 ## Residual 0.27495 0.5244 ## Number of obs: 4720, groups: SubjectID, 115; Experiment, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.4566 0.0868 6.0453 5.26 0.00186 ** ## ConditionS 0.3446 0.0868 6.0453 3.97 0.00726 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW ## ConditionS 0.985 anova(m.ri1, m.ri2) ## refitting model(s) with ML (instead of REML) ## Data: data.md.red ## Models: ## m.ri1: Effect_Size ~ 0 + Condition + (1 | SubjectID) ## m.ri2: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri1 4 7735.1 7761.0 -3863.6 7727.1 ## m.ri2 5 7709.9 7742.2 -3850.0 7699.9 27.236 1 1.801e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.2 Alternatively add random intercepts by region m.ri3 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Region), data = data.md.red); summary(m.ri3) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Region) ## Data: data.md.red ## ## REML criterion at convergence: 6956.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.2660 -0.5955 -0.0606 0.5189 5.9541 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SubjectID (Intercept) 0.23985 0.4897 ## Region (Intercept) 0.04808 0.2193 ## Residual 0.22905 0.4786 ## Number of obs: 4720, groups: SubjectID, 115; Region, 20 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.47069 0.06773 59.02195 6.950 3.29e-09 *** ## ConditionS 0.35871 0.06773 59.02195 5.296 1.83e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW ## ConditionS 0.979 anova(m.ri1, m.ri3) ## refitting model(s) with ML (instead of REML) ## Data: data.md.red ## Models: ## m.ri1: Effect_Size ~ 0 + Condition + (1 | SubjectID) ## m.ri3: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri1 4 7735.1 7761.0 -3863.6 7727.1 ## m.ri3 5 6955.9 6988.2 -3473.0 6945.9 781.24 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.3 Now add random intercepts by both experiment and region m.ri4 = lmer(Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + (1 | Region), data = data.md.red); summary(m.ri4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + ## (1 | Region) ## Data: data.md.red ## ## REML criterion at convergence: 6924.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.2849 -0.5986 -0.0618 0.5179 5.9728 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SubjectID (Intercept) 0.19805 0.4450 ## Region (Intercept) 0.04806 0.2192 ## Experiment (Intercept) 0.03336 0.1827 ## Residual 0.22811 0.4776 ## Number of obs: 4720, groups: SubjectID, 115; Region, 20; Experiment, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.45689 0.09918 10.24873 4.607 0.00091 *** ## ConditionS 0.34492 0.09918 10.24873 3.478 0.00573 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW ## ConditionS 0.990 2.5.4 Does a random intercept by region improve the model beyond one with participant &amp; experiment? anova(m.ri2, m.ri4) ## refitting model(s) with ML (instead of REML) ## Data: data.md.red ## Models: ## m.ri2: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) ## m.ri4: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + ## m.ri4: (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri2 5 7709.9 7742.2 -3850.0 7699.9 ## m.ri4 6 6927.0 6965.8 -3457.5 6915.0 784.88 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.5 Does a random intercept by experiment improve the model beyond one with participant &amp; region? anova(m.ri3, m.ri4) ## refitting model(s) with ML (instead of REML) ## Data: data.md.red ## Models: ## m.ri3: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Region) ## m.ri4: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + ## m.ri4: (1 | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri3 5 6955.9 6988.2 -3473.0 6945.9 ## m.ri4 6 6927.0 6965.8 -3457.5 6915.0 30.874 1 2.754e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.6 Now add random slopes m.ris = lmer(Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red); summary(m.ris) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + ## Condition | Experiment) + (1 + Condition | Region) ## Data: data.md.red ## ## REML criterion at convergence: 6527 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.6493 -0.5807 -0.0683 0.5089 5.9141 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## SubjectID (Intercept) 0.229264 0.47882 ## ConditionS 0.101223 0.31816 -0.36 ## Region (Intercept) 0.034497 0.18573 ## ConditionS 0.006672 0.08168 0.79 ## Experiment (Intercept) 0.023741 0.15408 ## ConditionS 0.005286 0.07270 0.47 ## Residual 0.199429 0.44657 ## Number of obs: 4720, groups: SubjectID, 115; Region, 20; Experiment, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.45783 0.08837 8.54662 5.181 0.000683 *** ## ConditionS 0.33841 0.10926 11.06806 3.097 0.010086 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW ## ConditionS 0.904 2.5.7 Do random slopes help beyond random intercepts? anova(m.ris, m.ri4) ## refitting model(s) with ML (instead of REML) ## Data: data.md.red ## Models: ## m.ri4: Effect_Size ~ 0 + Condition + (1 | SubjectID) + (1 | Experiment) + ## m.ri4: (1 | Region) ## m.ris: Effect_Size ~ 0 + Condition + (1 + Condition | SubjectID) + (1 + ## m.ris: Condition | Experiment) + (1 + Condition | Region) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.ri4 6 6927.0 6965.8 -3457.5 6915.0 ## m.ris 12 6543.5 6621.0 -3259.8 6519.5 395.53 6 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.8 We also have hemisphere as a variable in our model 2.5.9 Note that below it’s not included in the random structure, although it could / should be m.ris2 = lmer(Effect_Size ~ 0 + Condition*Hemisphere + (1 + Condition | SubjectID) + (1 + Condition| Experiment) + (1 + Condition | Region), data = data.md.red); ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.00385056 ## (tol = 0.002, component 1) summary(m.ris2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## Effect_Size ~ 0 + Condition * Hemisphere + (1 + Condition | SubjectID) + ## (1 + Condition | Experiment) + (1 + Condition | Region) ## Data: data.md.red ## ## REML criterion at convergence: 6530.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.6823 -0.5788 -0.0650 0.5060 5.9356 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## SubjectID (Intercept) 0.229282 0.47883 ## ConditionS 0.101220 0.31815 -0.36 ## Region (Intercept) 0.034846 0.18667 ## ConditionS 0.005205 0.07214 0.80 ## Experiment (Intercept) 0.023759 0.15414 ## ConditionS 0.005295 0.07277 0.47 ## Residual 0.199429 0.44657 ## Number of obs: 4720, groups: SubjectID, 115; Region, 20; Experiment, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## ConditionW 0.49670 0.09827 12.08824 5.055 0.000276 *** ## ConditionS 0.42000 0.12217 15.32320 3.438 0.003566 ** ## HemisphereR -0.07774 0.08548 17.98669 -0.909 0.375177 ## ConditionS:HemisphereR -0.08544 0.04144 17.98440 -2.062 0.053956 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## CndtnW CndtnS HmsphR ## ConditionS 0.913 ## HemisphereR -0.435 -0.437 ## CndtnS:HmsR -0.223 -0.349 0.513 ## convergence code: 0 ## Model failed to converge with max|grad| = 0.00385056 (tol = 0.002, component 1) "]]
